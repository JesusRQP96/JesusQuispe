[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Jesus Quispe\n\nI’m a Economist (PUCP). I worked as analyst. I blog about statistics, programming, and data analytics applied in Actuarial science, Health economics and development economics.\nI have extensive experience with causal inference, regression analysis, spatial analysis, econometrics, actuarial analytics.\nI excel at applied statistics, especially developing new methods to analyze complex data sets.\nI love to collaborate with scientists and product teams on involved projects.\nYou can find my resume here. I’m currently based in Lima,Peru"
  },
  {
    "objectID": "Articles.html",
    "href": "Articles.html",
    "title": "Articles",
    "section": "",
    "text": "Analisis factorial\n\n\nCFA & EFA\n\n\nApplicacion del analisis factorial integrado\n\n\n\n\n\n\nJan 1, 2025\n\n\n18 min\n\n\n\n\n\n\n  \n\n\n\n\nBayesian variable selection\n\n\nDrivers of wage\n\n\nDescription of how to apply the bayesian variable selection approach\n\n\n\n\n\n\nMar 16, 2024\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nPlotting with Ggplot2:Time series\n\n\nTime series\n\n\nDescription of how make beauty time series plots\n\n\n\n\n\n\nJan 10, 2022\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nAprendizaje automatico usando Tidymodels\n\n\nMachine learning & Quantitative finance\n\n\nDescription of how to use tidymodels to training Machine learning models.\n\n\n\n\n\n\nJun 10, 2021\n\n\n14 min\n\n\n\n\n\n\n  \n\n\n\n\nPlotting with Ggplot2:Barplot\n\n\nBarplot\n\n\nDescription of how make beauty barplots\n\n\n\n\n\n\nMay 10, 2019\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Articles/2019-05-10_Beauty_ggplots/2019-05-10_Beauty_ggplots.html",
    "href": "Articles/2019-05-10_Beauty_ggplots/2019-05-10_Beauty_ggplots.html",
    "title": "Plotting with Ggplot2:Barplot",
    "section": "",
    "text": "Plot beauty barplots\nWe had data of admissions to UCB\n\nlibrary(tidyverse)\nlibrary(knitr)\n\n\ndb.df&lt;-data.frame(UCBAdmissions)\n\nkable(db.df)\n\n\n\n\nAdmit\nGender\nDept\nFreq\n\n\n\n\nAdmitted\nMale\nA\n512\n\n\nRejected\nMale\nA\n313\n\n\nAdmitted\nFemale\nA\n89\n\n\nRejected\nFemale\nA\n19\n\n\nAdmitted\nMale\nB\n353\n\n\nRejected\nMale\nB\n207\n\n\nAdmitted\nFemale\nB\n17\n\n\nRejected\nFemale\nB\n8\n\n\nAdmitted\nMale\nC\n120\n\n\nRejected\nMale\nC\n205\n\n\nAdmitted\nFemale\nC\n202\n\n\nRejected\nFemale\nC\n391\n\n\nAdmitted\nMale\nD\n138\n\n\nRejected\nMale\nD\n279\n\n\nAdmitted\nFemale\nD\n131\n\n\nRejected\nFemale\nD\n244\n\n\nAdmitted\nMale\nE\n53\n\n\nRejected\nMale\nE\n138\n\n\nAdmitted\nFemale\nE\n94\n\n\nRejected\nFemale\nE\n299\n\n\nAdmitted\nMale\nF\n22\n\n\nRejected\nMale\nF\n351\n\n\nAdmitted\nFemale\nF\n24\n\n\nRejected\nFemale\nF\n317\n\n\n\n\n\nA first approach\n\ndb.df                                                   %&gt;% \nggplot()                                                 +\ngeom_col(aes(Admit,Freq,fill =Gender),\n         position = position_stack(reverse = FALSE),\n         lwd = 3)     +\nfacet_wrap(vars(Dept)) +\ntheme_bw()+\nxlab(\"Admition\") +\nylab(\"Frequency\")\n\n\n\n\nPlot upgraded\n\ndb.df                                                   %&gt;% \nggplot()                                                 +\ngeom_col(aes(Admit,Freq,fill =Gender,color=Gender),alpha=0.5,\n         position = position_stack(reverse = FALSE),\n         lwd = 1.2)     +\nfacet_wrap(vars(Dept)) +\nxlab(\"Admition\") +\nylab(\"Frequency\")+\nscale_fill_viridis_d() +\nscale_color_viridis_d()+\ntheme_dark()"
  },
  {
    "objectID": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html",
    "href": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html",
    "title": "Aprendizaje automatico usando Tidymodels",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(gt)\n\n\n\n\nUno de los modelos clasicos para llevar acabo el pricing de opciones es el modelo black-sholes el cual tipicamente plantea la siguiente entidad :\n\\[ C(S_0,t) = S_0 N(d_1) - Ke^{-r(T-t)}N(d_2)\\]\n\\[ d_1 =     \\frac{(ln\\frac{S_{0}}{K} + (r +\\frac{σ^2}{2} )(T-t))}{σ    \\sqrt{T-t}} \\]\n\\[d_2 = d_1 - σ \\sqrt{T-t}\\]\nDonde:\n\n\\(S_0\\): Precio del subyacente (Stock Price)\n\\(C(S0,t)\\): Price of the Call Option\n\\(K\\): Exercise Price\n\\((T−t)\\): Tiempo de maduracion, donde T es la fecha de ejercicio(Time to Maturity, where T is Exercise Date)\n\\(σ\\): Volatilidad subyacente (Underlying Volatility (a standard deviation of log returns))\n\\(r\\): Tasa de interes libre de riesgo (Risk-free Interest Rate (i.e., T-bill Rate))\n\nEsta ecuacion puede ser formulada en R del siguiente modo :\n\nblack_scholes_price &lt;- function(S, K = 70, r = 0, T = 1, sigma = 0.2) {\n  \n  d1     &lt;- (log(S / K) + (r + sigma^2 / 2) * T) / (sigma * sqrt(T))\n  d2     &lt;-  d1 - sigma * sqrt(T)\n  price  &lt;-  S * pnorm(d1) - K * exp(-r * T) * pnorm(d2)\n  \n  return(price)\n}\n\nEmpleando la anterior expresion, podemos simular precios de opciones;asi como una base de datos de sus determinantes. Dado lo anterior, tendremos un escenario perfecto para llevar acabo metodologias asociadas al aprendizaje automatico o ML ya que podriamos calcular el predecir el precio de una opcion financiera . Asi partimos simulando la base de datos necesaria para el problema de regresion que se nos presenta\n\nset.seed(420)\n\noption_prices &lt;- expand_grid(\n  S = 40:50,\n  K = 20:40,\n  r = seq(from = 0, to = 0.03, by = 0.01),\n  T = seq(from = 3 / 12, to = 1.5, by = 1 / 12),\n  sigma = seq(from = 0.1, to = 0.3, by = 0.1)\n) |&gt;\n  mutate(\n    black_scholes = black_scholes_price(S, K, r, T, sigma),\n    observed_price = map_dbl(\n      black_scholes,\n      function(x) x + rnorm(1, sd = 0.15)\n    )\n  )\n\n  option_prices |&gt; \n  head(10)  |&gt;\n  gt() \n\n\n\n\n\n  \n    \n    \n      S\n      K\n      r\n      T\n      sigma\n      black_scholes\n      observed_price\n    \n  \n  \n    40\n20\n0\n0.2500000\n0.1\n20.00000\n20.04016\n    40\n20\n0\n0.2500000\n0.2\n20.00000\n19.85949\n    40\n20\n0\n0.2500000\n0.3\n20.00000\n20.08943\n    40\n20\n0\n0.3333333\n0.1\n20.00000\n19.95319\n    40\n20\n0\n0.3333333\n0.2\n20.00000\n20.05970\n    40\n20\n0\n0.3333333\n0.3\n20.00003\n19.92282\n    40\n20\n0\n0.4166667\n0.1\n20.00000\n19.91722\n    40\n20\n0\n0.4166667\n0.2\n20.00000\n19.93092\n    40\n20\n0\n0.4166667\n0.3\n20.00023\n19.55791\n    40\n20\n0\n0.5000000\n0.1\n20.00000\n19.88493\n  \n  \n  \n\n\n\n\nInmediatamente procedemos a establecer el conjunto de datos de entrenamiento y testeo. Asi como establecemos la metodologia de V-fold validacion cruzada sobre el conjunto de datos de entrenamiento\n\n# 40 -60% \nsplit         &lt;- initial_split(option_prices, prop = 0.40)\noption_train  &lt;- training(split)\noption_test   &lt;- testing(split)\n\n\n\n\nset.seed(123)\noption_folds &lt;- vfold_cv(option_train, v = 20)\n\n\n\n\nLuego procedemos a plantear los potenciales predictores del precio de la opcion; asi como el procesamiento basico de los mismo o Feature engineering .Para ello empleamos una ‘recipe’\n\nrec.option &lt;- recipe(observed_price ~ .,\n              data = option_prices\n              ) |&gt;\n  step_rm(black_scholes) |&gt;\n  step_normalize(all_predictors())\n\n\n\n\nDefinimos el set de modelos que emplearemos y evaluaremos su performance\n\nmars_model &lt;- mars(  num_terms = tune(),\n                       prod_degree = tune(),\n                       prune_method = tune())  |&gt;\n                set_engine(\"earth\")            |&gt;\n                set_mode(\"regression\")\n\n\nlibrary(bonsai)\nrf_model &lt;-   rand_forest(mtry  = tune(), \n                           min_n  = tune(), trees = 100)            |&gt;\n              set_engine(\"ranger\")    |&gt;\n              set_mode(\"regression\")\n\n\n\n\nlgbm_model &lt;-   boost_tree(learn_rate = tune(), stop_iter = tune(),\n                            trees = 100) %&gt;%\n                set_engine(\"lightgbm\", num_leaves = tune()) %&gt;%\n                set_mode(\"regression\")\n\n\n\n\n\nmars_wflow   &lt;- workflow(rec.option, mars_model)\nrf_wflow     &lt;- workflow(rec.option, rf_model)\nlgbm_wflow   &lt;- workflow(rec.option, lgbm_model)\n\nLuego procedemos a ajustar el modelo planteado sobre los ‘folds’ creados, mediante la metodologia de validacion cruzada de manera que se busca optimizar los hiperparametros de los modelos planteados\n\nset.seed(123)\nmars_time_grid &lt;- system.time(\n                  mars_res_grid &lt;- tune_grid(mars_wflow, option_folds, grid = 5)\n)\n\n\nset.seed(123)\nrf_time_grid&lt;- system.time(\n                  rf_res_grid &lt;- tune_grid(rf_wflow, option_folds, grid = 5)\n)\n\n\nset.seed(123)\nlgbm_time_grid &lt;- system.time(\n                  lgbm_res_grid &lt;- tune_grid(lgbm_wflow, option_folds, grid = 5)\n)\n\n\n\n\nProcedemos a seleccionar el mejor modelo empleando para ello la metrica de error de la raiz cuadrada media estandarizada o rsme( por sus siglas en ingles )\n\nrf_rmse       &lt;- rf_res_grid %&gt;%\n                 select_best(\"rmse\", maximize = FALSE)\n\nlgbm_rmse     &lt;- lgbm_res_grid %&gt;%\n                 select_best(\"rmse\", maximize = FALSE)\n\nmars_rmse     &lt;- mars_res_grid %&gt;%\n                 select_best(\"rmse\", maximize = FALSE)\n\n\n\n\nfinal_mars &lt;- finalize_workflow(\n              mars_wflow,\n              mars_rmse\n)\n\nfinal_rf &lt;- finalize_workflow(\n              rf_wflow,\n              rf_rmse\n)\n\nfinal_lgbm &lt;- finalize_workflow(\n              lgbm_wflow,\n              lgbm_rmse\n)\n\n\n\nFinalmente reorganizamos los resultados de las metricas de error para facilitar la seleccion del mejor modelo.\n\nm_mars_db  &lt;- last_fit(\n              final_mars,\n              split\n              ) %&gt;%\n              collect_metrics() %&gt;% \n              mutate(.model =\"mars\")\n\n\nm_rf_db  &lt;- last_fit(\n            final_rf,\n            split\n            ) %&gt;%\n            collect_metrics() %&gt;% \n              mutate(.model =\"Random.Forest\")\n\nm_lgbm_db  &lt;- last_fit(\n              final_lgbm,\n              split\n              ) %&gt;%\n              collect_metrics()%&gt;% \n              mutate(.model =\"Light.gbm\")\n\n\nmtrcs_db   &lt;-rbind(m_mars_db,\n                   m_rf_db,\n                   m_lgbm_db)\n\n\nmtrcs_db                                                    |&gt; \nselect(.metric,.estimate,.model)                            |&gt; \npivot_wider(names_from = .metric,values_from =  .estimate)  |&gt; \ngt()                                                        |&gt;\ntab_header(\n    title = md(\"**Metricas de error**\"),\n    subtitle = \"Pricing opciones\"\n  )                                                         |&gt;\n  tab_options(\n    table.background.color = \"white\",\n    column_labels.background.color = \"white\",\n    table.font.size = px(15),\n    column_labels.font.size = px(17),\n    row.striping.background_color = \"gray\",\n    heading.align = \"center\",\n    heading.title.font.size = px(20)\n  ) %&gt;% \n  opt_row_striping() \n\n\n\n\n\n  \n    \n      Metricas de error\n    \n    \n      Pricing opciones\n    \n    \n      .model\n      rmse\n      rsq\n    \n  \n  \n    mars\n1.5005607\n0.9455582\n    Random.Forest\n0.2127804\n0.9989135\n    Light.gbm\n5.0715766\n0.9861009\n  \n  \n  \n\n\n\n\n\n\n\nPreprocesamiento del error de prediccion de cada modelo\n\nout_of_sample_data &lt;- testing(split)                  |&gt;\n                      slice_sample(n = 10000)\n\ndim(out_of_sample_data)\n\n[1] 10000     7\n\npredictive_performance &lt;- final_mars                  |&gt;         # best workflow\n                          fit(data = training(split)) |&gt;         # fitted with full training dataset\n                          predict(out_of_sample_data) |&gt;           # predict \n                          rename(\"mars\" = .pred)      |&gt; \n                          bind_cols(\n                              final_rf                      |&gt;         \n                              fit(data = training(split))   |&gt;         \n                              predict(out_of_sample_data)   |&gt;            \n                              rename(\"Random.Forest\" = .pred) \n                            )                               |&gt; \n                          bind_cols(\n                              final_rf                      |&gt;         \n                              fit(data = training(split))   |&gt;         \n                              predict(out_of_sample_data)   |&gt;            \n                              rename(\"LGBM\" = .pred) \n                            )                               |&gt;\n  bind_cols(out_of_sample_data)                             |&gt;\n  pivot_longer(\"mars\":\"LGBM\", names_to = \"Model\")           |&gt;\n    mutate(\n    moneyness = (S - K),\n    pricing_error = abs(value - black_scholes)\n  )\n\nVisualizacion final del error de prediccion de los modelos anteriormente evaluados\n\npredictive_performance |&gt;\n  ggplot(aes(\n    x = moneyness, \n    y = pricing_error, \n    color = Model,\n    linetype = Model\n    )) +\n  geom_jitter(alpha = 0.05) +\n  scale_color_viridis_d()+\n  geom_smooth(se = FALSE, method = \"gam\", formula = y ~ s(x, bs = \"cs\")) +\n  labs(\n    x = \"Moneyness (S - K)\", color = NULL,\n    y = \"Error de prediccion\",\n    title = \"Error de prediccion de modelos opciones de compra\",\n    linetype = NULL\n  )+theme_bw()+\n  theme(legend.position = \"bottom\",\n        plot.title = element_text(hjust = 0.5))"
  },
  {
    "objectID": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#carga-de-paquetes",
    "href": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#carga-de-paquetes",
    "title": "Aprendizaje automatico usando Tidymodels",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(gt)"
  },
  {
    "objectID": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#modelo-black--sholes",
    "href": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#modelo-black--sholes",
    "title": "Aprendizaje automatico usando Tidymodels",
    "section": "",
    "text": "Uno de los modelos clasicos para llevar acabo el pricing de opciones es el modelo black-sholes el cual tipicamente plantea la siguiente entidad :\n\\[ C(S_0,t) = S_0 N(d_1) - Ke^{-r(T-t)}N(d_2)\\]\n\\[ d_1 =     \\frac{(ln\\frac{S_{0}}{K} + (r +\\frac{σ^2}{2} )(T-t))}{σ    \\sqrt{T-t}} \\]\n\\[d_2 = d_1 - σ \\sqrt{T-t}\\]\nDonde:\n\n\\(S_0\\): Precio del subyacente (Stock Price)\n\\(C(S0,t)\\): Price of the Call Option\n\\(K\\): Exercise Price\n\\((T−t)\\): Tiempo de maduracion, donde T es la fecha de ejercicio(Time to Maturity, where T is Exercise Date)\n\\(σ\\): Volatilidad subyacente (Underlying Volatility (a standard deviation of log returns))\n\\(r\\): Tasa de interes libre de riesgo (Risk-free Interest Rate (i.e., T-bill Rate))\n\nEsta ecuacion puede ser formulada en R del siguiente modo :\n\nblack_scholes_price &lt;- function(S, K = 70, r = 0, T = 1, sigma = 0.2) {\n  \n  d1     &lt;- (log(S / K) + (r + sigma^2 / 2) * T) / (sigma * sqrt(T))\n  d2     &lt;-  d1 - sigma * sqrt(T)\n  price  &lt;-  S * pnorm(d1) - K * exp(-r * T) * pnorm(d2)\n  \n  return(price)\n}\n\nEmpleando la anterior expresion, podemos simular precios de opciones;asi como una base de datos de sus determinantes. Dado lo anterior, tendremos un escenario perfecto para llevar acabo metodologias asociadas al aprendizaje automatico o ML ya que podriamos calcular el predecir el precio de una opcion financiera . Asi partimos simulando la base de datos necesaria para el problema de regresion que se nos presenta\n\nset.seed(420)\n\noption_prices &lt;- expand_grid(\n  S = 40:50,\n  K = 20:40,\n  r = seq(from = 0, to = 0.03, by = 0.01),\n  T = seq(from = 3 / 12, to = 1.5, by = 1 / 12),\n  sigma = seq(from = 0.1, to = 0.3, by = 0.1)\n) |&gt;\n  mutate(\n    black_scholes = black_scholes_price(S, K, r, T, sigma),\n    observed_price = map_dbl(\n      black_scholes,\n      function(x) x + rnorm(1, sd = 0.15)\n    )\n  )\n\n  option_prices |&gt; \n  head(10)  |&gt;\n  gt() \n\n\n\n\n\n  \n    \n    \n      S\n      K\n      r\n      T\n      sigma\n      black_scholes\n      observed_price\n    \n  \n  \n    40\n20\n0\n0.2500000\n0.1\n20.00000\n20.04016\n    40\n20\n0\n0.2500000\n0.2\n20.00000\n19.85949\n    40\n20\n0\n0.2500000\n0.3\n20.00000\n20.08943\n    40\n20\n0\n0.3333333\n0.1\n20.00000\n19.95319\n    40\n20\n0\n0.3333333\n0.2\n20.00000\n20.05970\n    40\n20\n0\n0.3333333\n0.3\n20.00003\n19.92282\n    40\n20\n0\n0.4166667\n0.1\n20.00000\n19.91722\n    40\n20\n0\n0.4166667\n0.2\n20.00000\n19.93092\n    40\n20\n0\n0.4166667\n0.3\n20.00023\n19.55791\n    40\n20\n0\n0.5000000\n0.1\n20.00000\n19.88493\n  \n  \n  \n\n\n\n\nInmediatamente procedemos a establecer el conjunto de datos de entrenamiento y testeo. Asi como establecemos la metodologia de V-fold validacion cruzada sobre el conjunto de datos de entrenamiento\n\n# 40 -60% \nsplit         &lt;- initial_split(option_prices, prop = 0.40)\noption_train  &lt;- training(split)\noption_test   &lt;- testing(split)\n\n\n\n\nset.seed(123)\noption_folds &lt;- vfold_cv(option_train, v = 20)"
  },
  {
    "objectID": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#definicion-del-predictor",
    "href": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#definicion-del-predictor",
    "title": "Aprendizaje automatico usando Tidymodels",
    "section": "",
    "text": "Luego procedemos a plantear los potenciales predictores del precio de la opcion; asi como el procesamiento basico de los mismo o Feature engineering .Para ello empleamos una ‘recipe’\n\nrec.option &lt;- recipe(observed_price ~ .,\n              data = option_prices\n              ) |&gt;\n  step_rm(black_scholes) |&gt;\n  step_normalize(all_predictors())"
  },
  {
    "objectID": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#definicion-del-modelo-o-engine",
    "href": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#definicion-del-modelo-o-engine",
    "title": "Aprendizaje automatico usando Tidymodels",
    "section": "",
    "text": "Definimos el set de modelos que emplearemos y evaluaremos su performance\n\nmars_model &lt;- mars(  num_terms = tune(),\n                       prod_degree = tune(),\n                       prune_method = tune())  |&gt;\n                set_engine(\"earth\")            |&gt;\n                set_mode(\"regression\")\n\n\nlibrary(bonsai)\nrf_model &lt;-   rand_forest(mtry  = tune(), \n                           min_n  = tune(), trees = 100)            |&gt;\n              set_engine(\"ranger\")    |&gt;\n              set_mode(\"regression\")\n\n\n\n\nlgbm_model &lt;-   boost_tree(learn_rate = tune(), stop_iter = tune(),\n                            trees = 100) %&gt;%\n                set_engine(\"lightgbm\", num_leaves = tune()) %&gt;%\n                set_mode(\"regression\")"
  },
  {
    "objectID": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#definicion-de-los-flujos-de-trabajo",
    "href": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#definicion-de-los-flujos-de-trabajo",
    "title": "Aprendizaje automatico usando Tidymodels",
    "section": "",
    "text": "mars_wflow   &lt;- workflow(rec.option, mars_model)\nrf_wflow     &lt;- workflow(rec.option, rf_model)\nlgbm_wflow   &lt;- workflow(rec.option, lgbm_model)\n\nLuego procedemos a ajustar el modelo planteado sobre los ‘folds’ creados, mediante la metodologia de validacion cruzada de manera que se busca optimizar los hiperparametros de los modelos planteados\n\nset.seed(123)\nmars_time_grid &lt;- system.time(\n                  mars_res_grid &lt;- tune_grid(mars_wflow, option_folds, grid = 5)\n)\n\n\nset.seed(123)\nrf_time_grid&lt;- system.time(\n                  rf_res_grid &lt;- tune_grid(rf_wflow, option_folds, grid = 5)\n)\n\n\nset.seed(123)\nlgbm_time_grid &lt;- system.time(\n                  lgbm_res_grid &lt;- tune_grid(lgbm_wflow, option_folds, grid = 5)\n)"
  },
  {
    "objectID": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#evaluacion-del-mejor-modelo",
    "href": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#evaluacion-del-mejor-modelo",
    "title": "Aprendizaje automatico usando Tidymodels",
    "section": "",
    "text": "Procedemos a seleccionar el mejor modelo empleando para ello la metrica de error de la raiz cuadrada media estandarizada o rsme( por sus siglas en ingles )\n\nrf_rmse       &lt;- rf_res_grid %&gt;%\n                 select_best(\"rmse\", maximize = FALSE)\n\nlgbm_rmse     &lt;- lgbm_res_grid %&gt;%\n                 select_best(\"rmse\", maximize = FALSE)\n\nmars_rmse     &lt;- mars_res_grid %&gt;%\n                 select_best(\"rmse\", maximize = FALSE)\n\n\n\n\nfinal_mars &lt;- finalize_workflow(\n              mars_wflow,\n              mars_rmse\n)\n\nfinal_rf &lt;- finalize_workflow(\n              rf_wflow,\n              rf_rmse\n)\n\nfinal_lgbm &lt;- finalize_workflow(\n              lgbm_wflow,\n              lgbm_rmse\n)\n\n\n\nFinalmente reorganizamos los resultados de las metricas de error para facilitar la seleccion del mejor modelo.\n\nm_mars_db  &lt;- last_fit(\n              final_mars,\n              split\n              ) %&gt;%\n              collect_metrics() %&gt;% \n              mutate(.model =\"mars\")\n\n\nm_rf_db  &lt;- last_fit(\n            final_rf,\n            split\n            ) %&gt;%\n            collect_metrics() %&gt;% \n              mutate(.model =\"Random.Forest\")\n\nm_lgbm_db  &lt;- last_fit(\n              final_lgbm,\n              split\n              ) %&gt;%\n              collect_metrics()%&gt;% \n              mutate(.model =\"Light.gbm\")\n\n\nmtrcs_db   &lt;-rbind(m_mars_db,\n                   m_rf_db,\n                   m_lgbm_db)\n\n\nmtrcs_db                                                    |&gt; \nselect(.metric,.estimate,.model)                            |&gt; \npivot_wider(names_from = .metric,values_from =  .estimate)  |&gt; \ngt()                                                        |&gt;\ntab_header(\n    title = md(\"**Metricas de error**\"),\n    subtitle = \"Pricing opciones\"\n  )                                                         |&gt;\n  tab_options(\n    table.background.color = \"white\",\n    column_labels.background.color = \"white\",\n    table.font.size = px(15),\n    column_labels.font.size = px(17),\n    row.striping.background_color = \"gray\",\n    heading.align = \"center\",\n    heading.title.font.size = px(20)\n  ) %&gt;% \n  opt_row_striping() \n\n\n\n\n\n  \n    \n      Metricas de error\n    \n    \n      Pricing opciones\n    \n    \n      .model\n      rmse\n      rsq\n    \n  \n  \n    mars\n1.5005607\n0.9455582\n    Random.Forest\n0.2127804\n0.9989135\n    Light.gbm\n5.0715766\n0.9861009\n  \n  \n  \n\n\n\n\n\n\n\nPreprocesamiento del error de prediccion de cada modelo\n\nout_of_sample_data &lt;- testing(split)                  |&gt;\n                      slice_sample(n = 10000)\n\ndim(out_of_sample_data)\n\n[1] 10000     7\n\npredictive_performance &lt;- final_mars                  |&gt;         # best workflow\n                          fit(data = training(split)) |&gt;         # fitted with full training dataset\n                          predict(out_of_sample_data) |&gt;           # predict \n                          rename(\"mars\" = .pred)      |&gt; \n                          bind_cols(\n                              final_rf                      |&gt;         \n                              fit(data = training(split))   |&gt;         \n                              predict(out_of_sample_data)   |&gt;            \n                              rename(\"Random.Forest\" = .pred) \n                            )                               |&gt; \n                          bind_cols(\n                              final_rf                      |&gt;         \n                              fit(data = training(split))   |&gt;         \n                              predict(out_of_sample_data)   |&gt;            \n                              rename(\"LGBM\" = .pred) \n                            )                               |&gt;\n  bind_cols(out_of_sample_data)                             |&gt;\n  pivot_longer(\"mars\":\"LGBM\", names_to = \"Model\")           |&gt;\n    mutate(\n    moneyness = (S - K),\n    pricing_error = abs(value - black_scholes)\n  )\n\nVisualizacion final del error de prediccion de los modelos anteriormente evaluados\n\npredictive_performance |&gt;\n  ggplot(aes(\n    x = moneyness, \n    y = pricing_error, \n    color = Model,\n    linetype = Model\n    )) +\n  geom_jitter(alpha = 0.05) +\n  scale_color_viridis_d()+\n  geom_smooth(se = FALSE, method = \"gam\", formula = y ~ s(x, bs = \"cs\")) +\n  labs(\n    x = \"Moneyness (S - K)\", color = NULL,\n    y = \"Error de prediccion\",\n    title = \"Error de prediccion de modelos opciones de compra\",\n    linetype = NULL\n  )+theme_bw()+\n  theme(legend.position = \"bottom\",\n        plot.title = element_text(hjust = 0.5))"
  },
  {
    "objectID": "Articles/2022-01-10_Time_series/Time_series.html",
    "href": "Articles/2022-01-10_Time_series/Time_series.html",
    "title": "Plotting with Ggplot2:Time series",
    "section": "",
    "text": "Plotting Time series with ggplot\n#Packages\n\nlibrary(tidyverse)\n\n\nLoad Data\n\ndb.ts &lt;-  data.frame(discoveries) %&gt;% \n          mutate(year = 1860:1959)\n\n\n\nplot\n\ndb.ts                                       %&gt;% \nggplot(aes(year,discoveries))                +\ngeom_line(color=\"red4\",lwd=0.5)                      +\ngeom_point(color=\"red4\")                                 + \ntheme_bw()                                   +\nylab(\"Discoveries \")                                    +\nxlab(\" \")"
  },
  {
    "objectID": "Articles/2024-03-16_Bayesian_Variable_Selection/Bayesian Variable Selection.html",
    "href": "Articles/2024-03-16_Bayesian_Variable_Selection/Bayesian Variable Selection.html",
    "title": "Bayesian variable selection",
    "section": "",
    "text": "Load required packages\n\nlibrary(wooldridge)\nlibrary(spikeslab)\nlibrary(dplyr)\n\n\n\nPreprocess our data\n\ndata(wage1, package = \"wooldridge\")\n\nwage1 &lt;- wage1 |&gt;\n         select(-wage)\n\n\n\nFit spikeslab regression\nThe spcikeslab regression model enable us to rank the determinants of wage accord to bma and gnet. The calculus of these concepts permits us to select the best drivers in the next steps.\n\nset.seed(123)\nobj &lt;- spikeslab(lwage ~ . , wage1)\nprint(obj)\n\n------------------------------------------------------------------- \nVariable selection method     : AIC \nBig p small n                 : FALSE \nScreen variables              : FALSE \nFast processing               : TRUE \nSample size                   : 526 \nNo. predictors                : 22 \nNo. burn-in values            : 500 \nNo. sampled values            : 500 \nEstimated mse                 : 0.1323 \nModel size                    : 22 \n\n\n---&gt; Top variables:\n            bma   gnet bma.scale gnet.scale\neduc      0.131  0.131     0.047      0.047\nfemale   -0.126 -0.132    -0.252     -0.265\ntenure    0.118  0.154     0.016      0.021\nexper     0.117  0.268     0.009      0.020\ntrade    -0.107 -0.130    -0.237     -0.287\nexpersq  -0.100 -0.250     0.000      0.000\nprofocc   0.100  0.106     0.207      0.219\nservices -0.070 -0.087    -0.231     -0.289\nsmsa      0.062  0.063     0.137      0.140\nservocc  -0.048 -0.038    -0.137     -0.110\nmarried   0.044  0.033     0.091      0.068\ntenursq  -0.025 -0.066     0.000      0.000\nwest      0.024  0.022     0.064      0.060\nnorthcen -0.016 -0.023    -0.038     -0.053\nndurman  -0.013 -0.029    -0.039     -0.090\nsouth    -0.010 -0.017    -0.021     -0.037\nprofserv -0.008 -0.032    -0.017     -0.073\nnumdep   -0.005 -0.019    -0.004     -0.015\ntrcommpu -0.005 -0.016    -0.024     -0.077\nclerocc   0.001  0.010     0.003      0.027\nconstruc -0.001 -0.008    -0.004     -0.039\nnonwhite  0.000 -0.001    -0.001     -0.004\n------------------------------------------------------------------- \n\n\nWe can improve this approach using CV-validation .\n\ncv.obj &lt;- cv.spikeslab(x = wage1 %&gt;% select(-lwage), y = wage1$lwage, K = 10)\n\n     K-fold: 1 \n     K-fold: 2 \n     K-fold: 3 \n     K-fold: 4 \n     K-fold: 5 \n     K-fold: 6 \n     K-fold: 7 \n     K-fold: 8 \n     K-fold: 9 \n     K-fold: 10 \n     final analysis (full-data)\n\n\n\n\n\n------------------------------------------------------------------- \nVariable selection method     : cross-validation \nBig p small n                 : FALSE \nScreen variables              : FALSE \nFast processing               : TRUE \nSample size                   : 526 \nNo. predictors                : 22 \nNo. burn-in values            : 500 \nNo. sampled values            : 500 \nK-fold                        : 10 \nCV mean-squared error         : 0.139 +/- 0.009 \nModel size                    : [12,23] \n\n\nTop variables in terms of stability:\n            bma bma.cv   gnet gnet.cv stability\neduc      0.131  0.131  0.131   0.131       100\nfemale   -0.126 -0.125 -0.133  -0.129       100\ntenure    0.113  0.118  0.151   0.138       100\ntrade    -0.106 -0.108 -0.129  -0.119       100\nprofocc   0.099  0.099  0.106   0.103       100\nservices -0.069 -0.070 -0.087  -0.078       100\nsmsa      0.062  0.061  0.063   0.060       100\nservocc  -0.049 -0.049 -0.038  -0.042       100\nmarried   0.044  0.044  0.033   0.036       100\nwest      0.024  0.024  0.023   0.023       100\nexper     0.122  0.112  0.272   0.206        90\nexpersq  -0.105 -0.095 -0.254  -0.189        90\ntenursq  -0.021 -0.026 -0.062  -0.048        90\nnorthcen -0.015 -0.017 -0.023  -0.017        90\nndurman  -0.011 -0.013 -0.028  -0.022        90\nsouth    -0.009 -0.010 -0.017  -0.013        90\nprofserv -0.007 -0.009 -0.031  -0.023        90\nnumdep   -0.006 -0.006 -0.020  -0.013        90\ntrcommpu -0.005 -0.006 -0.016  -0.012        80\nconstruc -0.001 -0.002 -0.008  -0.006        80\nclerocc   0.001  0.001  0.010   0.005        70\nnonwhite  0.000  0.000 -0.001  -0.001        70\n------------------------------------------------------------------- \n\n\nonce the model is fitted, we observed that we need only 10 drivers; on the other hand, the variables that contributes the most in terms of stability are the following :\n\ncv.stb    &lt;- as.data.frame(cv.obj$stability)\ngnet      &lt;- cv.stb$gnet\nstability &lt;- cv.stb$stability\n\nplot(gnet,stability,bty=\"n\")\ntext(gnet, stability, rownames(cv.obj$stability), pos=1,cex = 0.8\n     )"
  },
  {
    "objectID": "Articles/2025-01-01_Factor_Analysis/analisis confirmatorio.html",
    "href": "Articles/2025-01-01_Factor_Analysis/analisis confirmatorio.html",
    "title": "Analisis factorial",
    "section": "",
    "text": "library(lavaan)\nlibrary(psych)\nlibrary(semTools)   #funcion para analisis de resultados,\nlibrary(tidyverse)\nlibrary(devtools)\nlibrary(readxl)\n\n#install.packages(\"skimr\")\nlibrary(skimr) # package\n\n#Improve L& f of tables\nlibrary(pander)\n\nlibrary(rio)\nlibrary(effectsize)\n\ndb_respuesta &lt;- read_excel(\"respuesta.xlsx\")\n\n\n\n\nSe emplea un estrategia en 3 pasos muchas veces empleada que integra el analisis factorial exploratorio y el analisis factorial confirmatorio (otras estrategias solo se emplea uno de los 2):\n\nExplorar la estructura factorial (Evaluacion de supuestos).\nConstruir el modelo factorial y evaluar el ajuste(estimacion EFA, CFA, criterios de informacion).\nEvaluar generabilidad (ajustar la muestra en los datos de prueba)\n\n\n\nLos resultados del modelo EFA, finalmente se validan a travez del modelo CFA, de modo tal que la evaluacion de supuestos de uno corresponde tambien la del otro\n\n\n\n\n\n\n\nskim(db_respuesta) %&gt;% \npander()\n\n\nTable continues below\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\nnumeric.mean\n\n\n\n\nnumeric\nTSC1\n0\n1\n3.653\n\n\nnumeric\nTSC2\n0\n1\n3.809\n\n\nnumeric\nTSC3\n0\n1\n3.732\n\n\nnumeric\nTSC4\n0\n1\n3.709\n\n\nnumeric\nTSC5\n0\n1\n3.822\n\n\nnumeric\nTE1\n0\n1\n4.061\n\n\nnumeric\nTE2\n0\n1\n4.043\n\n\nnumeric\nTE3\n0\n1\n4.121\n\n\nnumeric\nTE4\n0\n1\n4.105\n\n\nnumeric\nTE5\n0\n1\n3.902\n\n\nnumeric\nEE1\n0\n1\n3.812\n\n\nnumeric\nEE2\n0\n1\n3.727\n\n\nnumeric\nEE3\n0\n1\n3.878\n\n\nnumeric\nEE4\n0\n1\n3.687\n\n\nnumeric\nEE5\n0\n1\n3.987\n\n\nnumeric\nDE1\n0\n1\n3.925\n\n\nnumeric\nDE2\n0\n1\n3.596\n\n\nnumeric\nDE3\n0\n1\n3.817\n\n\nnumeric\nRPA1\n0\n1\n3.933\n\n\nnumeric\nRPA2\n0\n1\n3.941\n\n\nnumeric\nRPA3\n0\n1\n3.884\n\n\nnumeric\nRPA4\n0\n1\n3.869\n\n\nnumeric\nRPA5\n0\n1\n3.842\n\n\n\n\nTable continues below\n\n\n\n\n\n\n\n\n\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\n\n\n\n\n0.6849\n1\n3\n4\n4\n\n\n0.6389\n2\n3\n4\n4\n\n\n0.6396\n2\n3\n4\n4\n\n\n0.6734\n2\n3\n4\n4\n\n\n0.6522\n2\n3\n4\n4\n\n\n0.7122\n1\n4\n4\n5\n\n\n0.698\n2\n4\n4\n5\n\n\n0.7052\n1\n4\n4\n5\n\n\n0.6898\n1\n4\n4\n5\n\n\n0.7541\n1\n3\n4\n4\n\n\n0.7589\n1\n3\n4\n4\n\n\n0.8484\n1\n3\n4\n4\n\n\n0.828\n1\n3\n4\n4\n\n\n0.7959\n1\n3\n4\n4\n\n\n0.8105\n1\n3\n4\n5\n\n\n0.677\n1\n4\n4\n4\n\n\n0.682\n1\n3\n4\n4\n\n\n0.6984\n1\n3\n4\n4\n\n\n0.8343\n1\n3\n4\n5\n\n\n0.8049\n1\n4\n4\n4\n\n\n0.7886\n1\n3\n4\n4\n\n\n0.7649\n1\n3\n4\n4\n\n\n0.7858\n1\n3\n4\n4\n\n\n\n\n\n\n\n\n\n\nnumeric.p100\nnumeric.hist\n\n\n\n\n5\n▁▁▆▇▂\n\n\n5\n▁▃▁▇▂\n\n\n5\n▁▅▁▇▁\n\n\n5\n▁▅▁▇▂\n\n\n5\n▁▃▁▇▂\n\n\n5\n▁▁▂▇▃\n\n\n5\n▁▃▁▇▃\n\n\n5\n▁▁▂▇▅\n\n\n5\n▁▁▂▇▃\n\n\n5\n▁▁▃▇▃\n\n\n5\n▁▁▅▇▂\n\n\n5\n▁▁▆▇▃\n\n\n5\n▁▁▅▇▅\n\n\n5\n▁▁▇▇▃\n\n\n5\n▁▁▅▇▅\n\n\n5\n▁▁▂▇▂\n\n\n5\n▁▁▆▇▁\n\n\n5\n▁▁▅▇▂\n\n\n5\n▁▁▅▇▅\n\n\n5\n▁▁▃▇▃\n\n\n5\n▁▁▃▇▃\n\n\n5\n▁▁▃▇▃\n\n\n5\n▁▁▃▇▃\n\n\n\n\n\n\n\n\n\nRule of thumb &gt; 200 registros\n\n\n\nUn primer requerimiento antes de comenzar con el analisis factorial, es determinar el nivel de medicion;es decir, si son variables ordinales, cuantitativas, nominales, ya que la estructura de correlacion es la base del analisis factorial de modo tal que dependiendo de nivel de medicion sera necesario emplear una correlacion: pearson,tetracoricas,policoricas.\nVariables con similar rango y continuas\nPor defecto muchos paquetes en R asumen que son variables cuantitativas, variables obsersables con &gt;5 niveles.\n\n\n\n\n  db.longer_normal &lt;-   db_respuesta                                                                  |&gt; \n                        select(where(is.numeric))                                                     |&gt; \n                        pivot_longer(cols = everything(), names_to = \"Variable_obs\", values_to = \"Valor\") \n\n  db.longer_normal     |&gt; \n  ggplot(aes(x = Valor)) +\n  geom_histogram(bins = 30, fill = \"steelblue\", color = \"white\", alpha = 0.7) +\n  facet_wrap(~ Variable_obs, scales = \"free\") +  # Crear subgráficos por variable\n  theme_minimal() +\n  labs(title = \"Distribución de Variables Numéricas\", x = \"Variable Obs\", y = \"Frecuencia\")\n\n\n\n\n\n\n\n\nHay reglas of generales(rules of thumb), que establecen que ambos estadisticos no deben separar 3\n\n\ne1071skimrpyschpysch + pander\n\n\n\nlibrary(e1071)\n\ntbl.stats   &lt;-  db.longer_normal                                            |&gt;\n                group_by(Variable_obs)                                      |&gt;\n                summarise(\n                  skewness = e1071::skewness(Valor, na.rm = TRUE),\n                  kurtosis = e1071::kurtosis(Valor, na.rm = TRUE)\n                )\nlibrary(gt)  \n  \n  \n  tbl.stats                             |&gt;\n  gt()                                  |&gt;\n  data_color(\n    columns = vars(skewness),\n    colors = scales::col_bin(\n      bins = c(-Inf, 0, Inf),\n      palette = c(\"coral\", \"aliceblue\")\n    )\n  )                                     |&gt;\n  data_color(\n    columns = vars(kurtosis),\n    colors = scales::col_bin(\n      bins = c(-Inf, 0, Inf),\n      palette = c(\"coral\", \"aliceblue\")\n    )\n  )                                     |&gt;\n  tab_header(\n    title = \"Asimetría y Curtosis por Variable\"\n  )\n\n\n\n\n\n  \n    \n      Asimetría y Curtosis por Variable\n    \n    \n    \n      Variable_obs\n      skewness\n      kurtosis\n    \n  \n  \n    DE1\n-0.52605215\n1.24559696\n    DE2\n-0.21897007\n0.63925394\n    DE3\n-0.13677255\n0.01245932\n    EE1\n-0.34581133\n0.23143832\n    EE2\n-0.36964012\n0.12256023\n    EE3\n-0.31280354\n-0.39901657\n    EE4\n-0.03410964\n-0.40742547\n    EE5\n-0.42732927\n-0.26944283\n    RPA1\n-0.59305403\n0.50420600\n    RPA2\n-0.78552074\n1.22272528\n    RPA3\n-0.58790017\n0.74890573\n    RPA4\n-0.47810685\n0.33329229\n    RPA5\n-0.53468885\n0.67055640\n    TE1\n-0.46692932\n0.37701716\n    TE2\n-0.22002197\n-0.44900737\n    TE3\n-0.72100623\n1.60041195\n    TE4\n-0.47367810\n0.50711668\n    TE5\n-0.41165968\n0.16474956\n    TSC1\n-0.09242437\n0.06296766\n    TSC2\n-0.07293765\n-0.14123339\n    TSC3\n-0.16580865\n-0.01842834\n    TSC4\n-0.02578558\n-0.25212689\n    TSC5\n-0.09955242\n-0.12934441\n  \n  \n  \n\n\n\n\n\n\n\nskim(db_respuesta)\n\n\nData summary\n\n\nName\ndb_respuesta\n\n\nNumber of rows\n876\n\n\nNumber of columns\n23\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n23\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nTSC1\n0\n1\n3.65\n0.68\n1\n3\n4\n4\n5\n▁▁▆▇▂\n\n\nTSC2\n0\n1\n3.81\n0.64\n2\n3\n4\n4\n5\n▁▃▁▇▂\n\n\nTSC3\n0\n1\n3.73\n0.64\n2\n3\n4\n4\n5\n▁▅▁▇▁\n\n\nTSC4\n0\n1\n3.71\n0.67\n2\n3\n4\n4\n5\n▁▅▁▇▂\n\n\nTSC5\n0\n1\n3.82\n0.65\n2\n3\n4\n4\n5\n▁▃▁▇▂\n\n\nTE1\n0\n1\n4.06\n0.71\n1\n4\n4\n5\n5\n▁▁▂▇▃\n\n\nTE2\n0\n1\n4.04\n0.70\n2\n4\n4\n5\n5\n▁▃▁▇▃\n\n\nTE3\n0\n1\n4.12\n0.71\n1\n4\n4\n5\n5\n▁▁▂▇▅\n\n\nTE4\n0\n1\n4.11\n0.69\n1\n4\n4\n5\n5\n▁▁▂▇▃\n\n\nTE5\n0\n1\n3.90\n0.75\n1\n3\n4\n4\n5\n▁▁▃▇▃\n\n\nEE1\n0\n1\n3.81\n0.76\n1\n3\n4\n4\n5\n▁▁▅▇▂\n\n\nEE2\n0\n1\n3.73\n0.85\n1\n3\n4\n4\n5\n▁▁▆▇▃\n\n\nEE3\n0\n1\n3.88\n0.83\n1\n3\n4\n4\n5\n▁▁▅▇▅\n\n\nEE4\n0\n1\n3.69\n0.80\n1\n3\n4\n4\n5\n▁▁▇▇▃\n\n\nEE5\n0\n1\n3.99\n0.81\n1\n3\n4\n5\n5\n▁▁▅▇▅\n\n\nDE1\n0\n1\n3.92\n0.68\n1\n4\n4\n4\n5\n▁▁▂▇▂\n\n\nDE2\n0\n1\n3.60\n0.68\n1\n3\n4\n4\n5\n▁▁▆▇▁\n\n\nDE3\n0\n1\n3.82\n0.70\n1\n3\n4\n4\n5\n▁▁▅▇▂\n\n\nRPA1\n0\n1\n3.93\n0.83\n1\n3\n4\n5\n5\n▁▁▅▇▅\n\n\nRPA2\n0\n1\n3.94\n0.80\n1\n4\n4\n4\n5\n▁▁▃▇▃\n\n\nRPA3\n0\n1\n3.88\n0.79\n1\n3\n4\n4\n5\n▁▁▃▇▃\n\n\nRPA4\n0\n1\n3.87\n0.76\n1\n3\n4\n4\n5\n▁▁▃▇▃\n\n\nRPA5\n0\n1\n3.84\n0.79\n1\n3\n4\n4\n5\n▁▁▃▇▃\n\n\n\n\n\n\n\n\npsych::describe(db_respuesta)\n\n     vars   n mean   sd median trimmed  mad min max range  skew kurtosis   se\nTSC1    1 876 3.65 0.68      4    3.62 0.00   1   5     4 -0.09     0.06 0.02\nTSC2    2 876 3.81 0.64      4    3.78 0.00   2   5     3 -0.07    -0.14 0.02\nTSC3    3 876 3.73 0.64      4    3.71 0.00   2   5     3 -0.17    -0.02 0.02\nTSC4    4 876 3.71 0.67      4    3.67 0.00   2   5     3 -0.03    -0.25 0.02\nTSC5    5 876 3.82 0.65      4    3.79 0.00   2   5     3 -0.10    -0.13 0.02\nTE1     6 876 4.06 0.71      4    4.10 0.00   1   5     4 -0.47     0.38 0.02\nTE2     7 876 4.04 0.70      4    4.07 0.00   2   5     3 -0.22    -0.45 0.02\nTE3     8 876 4.12 0.71      4    4.17 0.00   1   5     4 -0.72     1.60 0.02\nTE4     9 876 4.11 0.69      4    4.15 0.00   1   5     4 -0.47     0.51 0.02\nTE5    10 876 3.90 0.75      4    3.92 0.00   1   5     4 -0.41     0.16 0.03\nEE1    11 876 3.81 0.76      4    3.81 0.00   1   5     4 -0.35     0.23 0.03\nEE2    12 876 3.73 0.85      4    3.75 1.48   1   5     4 -0.37     0.12 0.03\nEE3    13 876 3.88 0.83      4    3.91 1.48   1   5     4 -0.31    -0.40 0.03\nEE4    14 876 3.69 0.80      4    3.67 1.48   1   5     4 -0.03    -0.41 0.03\nEE5    15 876 3.99 0.81      4    4.03 1.48   1   5     4 -0.43    -0.27 0.03\nDE1    16 876 3.92 0.68      4    3.93 0.00   1   5     4 -0.53     1.25 0.02\nDE2    17 876 3.60 0.68      4    3.58 1.48   1   5     4 -0.22     0.64 0.02\nDE3    18 876 3.82 0.70      4    3.79 0.00   1   5     4 -0.14     0.01 0.02\nRPA1   19 876 3.93 0.83      4    3.97 1.48   1   5     4 -0.59     0.50 0.03\nRPA2   20 876 3.94 0.80      4    3.99 0.00   1   5     4 -0.79     1.22 0.03\nRPA3   21 876 3.88 0.79      4    3.91 0.00   1   5     4 -0.59     0.75 0.03\nRPA4   22 876 3.87 0.76      4    3.89 0.00   1   5     4 -0.48     0.33 0.03\nRPA5   23 876 3.84 0.79      4    3.86 0.00   1   5     4 -0.53     0.67 0.03\n\n\n\n\n\npsych::describe(db_respuesta) %&gt;% \npander()\n\n\nTable continues below\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\n\n\n\n\nTSC1\n1\n876\n3.653\n0.6849\n4\n3.62\n0\n1\n5\n\n\nTSC2\n2\n876\n3.809\n0.6389\n4\n3.776\n0\n2\n5\n\n\nTSC3\n3\n876\n3.732\n0.6396\n4\n3.708\n0\n2\n5\n\n\nTSC4\n4\n876\n3.709\n0.6734\n4\n3.665\n0\n2\n5\n\n\nTSC5\n5\n876\n3.822\n0.6522\n4\n3.795\n0\n2\n5\n\n\nTE1\n6\n876\n4.061\n0.7122\n4\n4.098\n0\n1\n5\n\n\nTE2\n7\n876\n4.043\n0.698\n4\n4.066\n0\n2\n5\n\n\nTE3\n8\n876\n4.121\n0.7052\n4\n4.174\n0\n1\n5\n\n\nTE4\n9\n876\n4.105\n0.6898\n4\n4.148\n0\n1\n5\n\n\nTE5\n10\n876\n3.902\n0.7541\n4\n3.923\n0\n1\n5\n\n\nEE1\n11\n876\n3.812\n0.7589\n4\n3.815\n0\n1\n5\n\n\nEE2\n12\n876\n3.727\n0.8484\n4\n3.748\n1.483\n1\n5\n\n\nEE3\n13\n876\n3.878\n0.828\n4\n3.906\n1.483\n1\n5\n\n\nEE4\n14\n876\n3.687\n0.7959\n4\n3.672\n1.483\n1\n5\n\n\nEE5\n15\n876\n3.987\n0.8105\n4\n4.028\n1.483\n1\n5\n\n\nDE1\n16\n876\n3.925\n0.677\n4\n3.934\n0\n1\n5\n\n\nDE2\n17\n876\n3.596\n0.682\n4\n3.577\n1.483\n1\n5\n\n\nDE3\n18\n876\n3.817\n0.6984\n4\n3.795\n0\n1\n5\n\n\nRPA1\n19\n876\n3.933\n0.8343\n4\n3.974\n1.483\n1\n5\n\n\nRPA2\n20\n876\n3.941\n0.8049\n4\n3.991\n0\n1\n5\n\n\nRPA3\n21\n876\n3.884\n0.7886\n4\n3.913\n0\n1\n5\n\n\nRPA4\n22\n876\n3.869\n0.7649\n4\n3.893\n0\n1\n5\n\n\nRPA5\n23\n876\n3.842\n0.7858\n4\n3.863\n0\n1\n5\n\n\n\n\n\n\n\n\n\n\n\n\n\n \nrange\nskew\nkurtosis\nse\n\n\n\n\nTSC1\n4\n-0.09242\n0.06297\n0.02314\n\n\nTSC2\n3\n-0.07294\n-0.1412\n0.02159\n\n\nTSC3\n3\n-0.1658\n-0.01843\n0.02161\n\n\nTSC4\n3\n-0.02579\n-0.2521\n0.02275\n\n\nTSC5\n3\n-0.09955\n-0.1293\n0.02204\n\n\nTE1\n4\n-0.4669\n0.377\n0.02406\n\n\nTE2\n3\n-0.22\n-0.449\n0.02358\n\n\nTE3\n4\n-0.721\n1.6\n0.02383\n\n\nTE4\n4\n-0.4737\n0.5071\n0.02331\n\n\nTE5\n4\n-0.4117\n0.1647\n0.02548\n\n\nEE1\n4\n-0.3458\n0.2314\n0.02564\n\n\nEE2\n4\n-0.3696\n0.1226\n0.02866\n\n\nEE3\n4\n-0.3128\n-0.399\n0.02798\n\n\nEE4\n4\n-0.03411\n-0.4074\n0.02689\n\n\nEE5\n4\n-0.4273\n-0.2694\n0.02739\n\n\nDE1\n4\n-0.5261\n1.246\n0.02287\n\n\nDE2\n4\n-0.219\n0.6393\n0.02304\n\n\nDE3\n4\n-0.1368\n0.01246\n0.0236\n\n\nRPA1\n4\n-0.5931\n0.5042\n0.02819\n\n\nRPA2\n4\n-0.7855\n1.223\n0.0272\n\n\nRPA3\n4\n-0.5879\n0.7489\n0.02664\n\n\nRPA4\n4\n-0.4781\n0.3333\n0.02584\n\n\nRPA5\n4\n-0.5347\n0.6706\n0.02655\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEs necesario validar que existe suficiente correlacion entre las variables observadas,sino no tiene caso si quiera plantear la existencia de factores, para ellos se puede calcular el test de Bartlett, el cual testea :\nho : Matriz de correlacion es una matriz identidad (0’s fuera de la diagonal) h1 : no lo es\n\n# nobmres de columnas\nvarnames                 &lt;- colnames(db_respuesta)\n\n## Matriz de correlacion \nmtr.corr_resp            &lt;- cor(db_respuesta[,varnames])\n\nmtr.corr_bart_result     &lt;-psych::cortest.bartlett(R=mtr.corr_resp,n = nrow(db_respuesta))\n\nmtr.corr_bart_result$p.value&lt;0.05  # Mucho cuidado con n, dependiende de su dimensiodad varia de significativo o no \n\n[1] TRUE\n\n\n\n\n\n\nPara ello requerimos el kmo que se puede entender como la propocion de la varianza total, explicada por los factores\nEl estado kmo , en particular se establece los siguientes umbrales :\n\n\n\n\nKMO\nInterpretación\n\n\n\n\n≥ 0.90\nExcelente\n\n\n0.80–0.89\nMuy bueno\n\n\n0.70–0.79\nAceptable\n\n\n0.60–0.69\nMediocre\n\n\n0.50–0.59\nPobre\n\n\n&lt; 0.50\nInaceptable\n\n\n\n\nKMO_result &lt;- KMO(db_respuesta)\n\n\n\n\n\nSe recomienda tener una muestra para probar los resultados del modelo factorial, al respecto :\n\npara calcular el modelo factorial : n(tamaño muestra) = 5*numero de parametros\nconsiderar tener una muestra de testeo/ evaluacion para extrapolar/generalizar resultados : 2*n\n\n\n# tamaño de muestra mitad\nmitad_muestra     &lt;- nrow(db_respuesta)/2\n\nset.seed(19)\n\n## Vector para marcar datos\nvc_marca          &lt;- sample(c(rep(\"model.building\",mitad_muestra),\n                         rep(\"holdout\",mitad_muestra)\n                         ))\n\n## Dividir el vecto x, segun grupos definidos en el vector y \n\ndb_model.build_list   &lt;- split(db_respuesta,vc_marca)\n\n\ndb_train              &lt;-db_model.build_list$model.building\ndb_test               &lt;-db_model.build_list$holdout\n\n\n\n\n\n\n\nParte de un supuesto clave: Existen los factores, pero no se saben cuantos son ni que variables observables lo componen\nPuede ser realizada con 2 metodos, que no necesariamente se menoscaban, de hecho se complementan :\n\n\nAnalisis paralelo : Para detectar el numero de factores(ejm constructos)\nAnalisis de ajuste : Para detectar el ajuste del modelo con el numero de factores decidido en a) se puede calcular criterios de informacion ejm BIC\n\n\nparalel_results &lt;- fa.parallel(x =db_train,fa=\"fa\")\n\n\n\n\nParallel analysis suggests that the number of factors =  5  and the number of components =  NA \n\n# cargas factoriales : representan las correlaciones entre las variables obser y el factor latente, si los datos \n# estuvieran estadarizados\n\nvalue.factor       &lt;- paralel_results$fa.values    # factorial loadins\nvalue.factor.simil &lt;- paralel_results$fa.sim\nn.factor           &lt;- 1:length(value.factor)       # numero de factores\n\nscreen.data  &lt;- tibble(loading       = value.factor, \n                       loading_simul = value.factor.simil,\n                       n_factor      = n.factor)\n\n\nScreen plotAnalisis paralelo\n\n\n\n# Idealmente se espera cargas superior a 0.5 (gpt)\nscreen.data                                                     |&gt;\nggplot(aes(n.factor,value.factor))                              +\ngeom_line(size = 1, \n            color = \"lightblue\")                                +\ngeom_hline(yintercept = 0.5, linetype = \"dashed\",color = \"red\") +\ngeom_point(color=\"cornflowerblue\", \n             size = 2, \n             alpha=.8)                                          +\ntheme_bw()                                                      +\nxlab(\"Numero de factores\")                                      +\nylab(\"carga factorial\")\n\n\n\n\n\n\n\n## Analisis paralelo\nscreen.data                        |&gt;\nggplot() +\ngeom_line(aes(n.factor,value.factor),\n            size = 1, \n            color = \"lightblue\")   +\ngeom_point(aes(n.factor,value.factor),\n             color=\"cornflowerblue\", \n             size = 2, \n             alpha=.8)             +\ngeom_line(aes(n.factor,value.factor.simil),\n            size = 1, \n            color = \"darkred\")   +\ngeom_point(aes(n.factor,value.factor.simil),\n             color=\"darkred\", \n             size = 2, \n             alpha=.8)             +\nggtitle(\"Analisis paralelo\")       +\ntheme_bw()                         +\nxlab(\"Numero de factores\")         +\nylab(\"carga factorial\")\n\n\n\n\n\n\n\nEsta metodologia solo dice un numero plausible de factores, mas no el numero final(lo mejor es considerar el rango que lo limita) , el numero final de factores debe ser decidido finalmente en base a : la interpretabilidad y el ajuste\n\n\nLa varianza de los items tiene 3 componentes : varianza compartida, especifica, y varianza del error - comunalidad,especificidad y varianza residual .\nPara estimar un modelo asi se debe especificar: * El numero de factores * El metodo de estimacion * la rotacion de la matriz\nAsi para estimar el modelo hay 3 principales tecnicas: * ML * PCA * PAF\n\nresults.efa &lt;- lavaan::efa(data      = db_train,\n                           nfactors  = 4:6,     # - el numero otorgado por el analisis paralelo otorga\n                                                #   el rango, el cual seria le numero de factores adecuado.\n                           rotation  = \"geomin\",# - se escoge este metodo de rotacion xq permite que haya correlacion\n                                                # entre factores y tiene sentido pues se habla de constructos educativos\n                           estimator =\"MLR\",    # - Preferible metodo de estimacion al ML, por posibles violaciones del\n                                                # supuesto de normalidad\n                           meanstructure = TRUE)\n\n\n\nDeacuerdo la metricas de los criterios de criterios de informacion queda claro que el mejor terminar ajustando es el modelo de 5 factores\n\nsort(lavaan::fitmeasures(results.efa)[\"bic\",])\n\nnfactors = 5 nfactors = 4 nfactors = 6 \n    18142.38     18167.49     18189.29 \n\n\n\n\n\nel paquete lavaan te otorga las cargas estandarizadas de modo tal que representarian la correlacion entre la variable observada y el factor\n\nresults.efa$nf5\n\n\n         f1      f2      f3      f4      f5 \nTSC1  0.584*                       *      .*\nTSC2  0.487*                       *      .*\nTSC3  0.637*                      .*       *\nTSC4  0.578*      .*              .*       *\nTSC5  0.547*                              . \nTE1           0.728*              .         \nTE2       .   0.672*                        \nTE3           0.708*      .                 \nTE4           0.651*              .*        \nTE5           0.337*      .*      .*        \nEE1               .   0.469*      .         \nEE2       .*          0.689*                \nEE3                   0.768*                \nEE4       .*          0.732*              . \nEE5               .   0.479*      .*        \nDE1                  -0.353*  0.744*      . \nDE2               .*          0.821*        \nDE3                       .*  0.755*        \nRPA1                                  0.851*\nRPA2                                  0.906*\nRPA3                                  0.624*\nRPA4                      .       .   0.350*\nRPA5                      .       .   0.338*\n\n\n\n\n\n\n\ndb_train_sumas &lt;- db_train %&gt;%\n                mutate(\n                  # Aplicamos across con una expresión lógica para cada grupo\n                  TSC = rowSums(across(matches(\"^TSC\\\\d+\")), na.rm = TRUE),\n                  EE  = rowSums(across(matches(\"^TE\\\\d+\")),  na.rm = TRUE),\n                  TE  = rowSums(across(matches(\"^EE\\\\d+\")),  na.rm = TRUE),\n                  DE  = rowSums(across(matches(\"^DE\\\\d+\")),  na.rm = TRUE),\n                  RPA = rowSums(across(matches(\"^RPA\\\\d+\")), na.rm = TRUE)\n                )\n\n\n#install.packages(\"BayesFM\")\nlibrary(\"BayesFM\")\n\n# specify model\nmodel &lt;- c(                                        # X1 covariate in all equations\n            paste0('TSC',' ~ ','TSC',1:5),         # X2 covariate for Y1-Y5 only\n            paste0('TE',' ~ ','TE',6:10),          # X3 covariate for Y6-Y10 only\n            paste0('EE',' ~ ','EE',11:15))         # X4 covariate for Y11-Y15 only\n\nmodel &lt;- lapply(model, as.formula) # make list of formulas\n\n# run MCMC sampler, post process and summarize\nmcmc &lt;- befa(model, data = db_train, Kmax = 5, iter = 1000)\n\n\n\n\n\nEste analisis pone a prueba la estructura establecida, para ello usamos la sintaxis lavaan de modelo, tal que asi:\n\nCFA_model &lt;- '\n  # Regressing items on factors,ejm el componente TSC es medido por TSC1 + TSC2 + TSC3 + TSC5\n  TSC =~ TSC1 + TSC2 + TSC3 + TSC5\n  TE  =~ TE1  + TE2  + TE3  + TE5\n  EE  =~ EE1  + EE2  + EE3  + EE4\n  DE  =~ DE1  + DE2  + DE3\n  RPA =~ RPA1 + RPA2 + RPA3 + RPA4\n\n  # Se permite correlacion entre factores\n  TSC ~~ TE\n  TSC ~~ EE\n  TSC ~~ DE\n  TSC ~~ RPA\n\n  TE ~~ EE\n  TE ~~ DE\n  TE ~~ RPA\n\n  EE ~~ DE\n  EE ~~ RPA\n\n  DE ~~ RPA\n'\n\nCFA.results.train &lt;- cfa( model         = CFA_model,\n                         data          = db_train,\n                         estimator     = \"MLR\",\n                         std.lv        = TRUE,\n                         meanstructure = TRUE)\n\n\n\n\nstd.all Corresponde a las cargas factoriales de cada variable observable en cada contructo\n\n\nA) DefaultB) Parameters packageC) Grafico del analisis confirmatorio\n\n\n\n# referenica : https://www.youtube.com/watch?v=GO5lCfHiFzE***\n\nsummary(CFA.results.train,standardized=T,fit.measures=T)\n\nlavaan 0.6-19 ended normally after 35 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        67\n\n  Number of observations                           438\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               352.092     318.841\n  Degrees of freedom                               142         142\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  1.104\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              4069.230    3549.780\n  Degrees of freedom                               171         171\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.146\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.946       0.948\n  Tucker-Lewis Index (TLI)                       0.935       0.937\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.950\n  Robust Tucker-Lewis Index (TLI)                            0.939\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -7277.376   -7277.376\n  Scaling correction factor                                  1.177\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -7101.330   -7101.330\n  Scaling correction factor                                  1.128\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               14688.751   14688.751\n  Bayesian (BIC)                             14962.260   14962.260\n  Sample-size adjusted Bayesian (SABIC)      14749.636   14749.636\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.058       0.053\n  90 Percent confidence interval - lower         0.051       0.046\n  90 Percent confidence interval - upper         0.066       0.061\n  P-value H_0: RMSEA &lt;= 0.050                    0.040       0.224\n  P-value H_0: RMSEA &gt;= 0.080                    0.000       0.000\n                                                                  \n  Robust RMSEA                                               0.056\n  90 Percent confidence interval - lower                     0.048\n  90 Percent confidence interval - upper                     0.064\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.110\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.044       0.044\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  TSC =~                                                                \n    TSC1              0.428    0.029   14.606    0.000    0.428    0.657\n    TSC2              0.430    0.028   15.320    0.000    0.430    0.692\n    TSC3              0.396    0.033   11.909    0.000    0.396    0.628\n    TSC5              0.486    0.029   16.492    0.000    0.486    0.726\n  TE =~                                                                 \n    TE1               0.567    0.034   16.507    0.000    0.567    0.789\n    TE2               0.519    0.027   19.357    0.000    0.519    0.745\n    TE3               0.538    0.039   13.883    0.000    0.538    0.788\n    TE5               0.479    0.041   11.774    0.000    0.479    0.649\n  EE =~                                                                 \n    EE1               0.560    0.035   16.046    0.000    0.560    0.739\n    EE2               0.664    0.033   19.879    0.000    0.664    0.802\n    EE3               0.645    0.035   18.492    0.000    0.645    0.786\n    EE4               0.612    0.035   17.698    0.000    0.612    0.760\n  DE =~                                                                 \n    DE1               0.444    0.042   10.595    0.000    0.444    0.665\n    DE2               0.446    0.040   11.159    0.000    0.446    0.640\n    DE3               0.537    0.036   14.749    0.000    0.537    0.738\n  RPA =~                                                                \n    RPA1              0.683    0.035   19.474    0.000    0.683    0.849\n    RPA2              0.670    0.037   17.951    0.000    0.670    0.854\n    RPA3              0.623    0.040   15.403    0.000    0.623    0.788\n    RPA4              0.435    0.043   10.064    0.000    0.435    0.587\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  TSC ~~                                                                \n    TE                0.659    0.045   14.688    0.000    0.659    0.659\n    EE                0.766    0.036   21.496    0.000    0.766    0.766\n    DE                0.564    0.060    9.374    0.000    0.564    0.564\n    RPA               0.686    0.037   18.725    0.000    0.686    0.686\n  TE ~~                                                                 \n    EE                0.689    0.041   16.816    0.000    0.689    0.689\n    DE                0.496    0.059    8.452    0.000    0.496    0.496\n    RPA               0.650    0.044   14.633    0.000    0.650    0.650\n  EE ~~                                                                 \n    DE                0.440    0.061    7.266    0.000    0.440    0.440\n    RPA               0.745    0.038   19.593    0.000    0.745    0.745\n  DE ~~                                                                 \n    RPA               0.340    0.059    5.742    0.000    0.340    0.340\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .TSC1              3.662    0.031  117.614    0.000    3.662    5.620\n   .TSC2              3.813    0.030  128.595    0.000    3.813    6.145\n   .TSC3              3.719    0.030  123.332    0.000    3.719    5.893\n   .TSC5              3.861    0.032  120.631    0.000    3.861    5.764\n   .TE1               4.064    0.034  118.349    0.000    4.064    5.655\n   .TE2               4.048    0.033  121.772    0.000    4.048    5.818\n   .TE3               4.103    0.033  125.699    0.000    4.103    6.006\n   .TE5               3.934    0.035  111.433    0.000    3.934    5.324\n   .EE1               3.808    0.036  105.097    0.000    3.808    5.022\n   .EE2               3.751    0.040   94.879    0.000    3.751    4.533\n   .EE3               3.884    0.039   99.026    0.000    3.884    4.732\n   .EE4               3.710    0.038   96.396    0.000    3.710    4.606\n   .DE1               3.934    0.032  123.361    0.000    3.934    5.894\n   .DE2               3.600    0.033  108.034    0.000    3.600    5.162\n   .DE3               3.808    0.035  109.535    0.000    3.808    5.234\n   .RPA1              3.947    0.038  102.635    0.000    3.947    4.904\n   .RPA2              3.963    0.037  105.764    0.000    3.963    5.054\n   .RPA3              3.893    0.038  103.011    0.000    3.893    4.922\n   .RPA4              3.904    0.035  110.386    0.000    3.904    5.274\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .TSC1              0.242    0.019   12.538    0.000    0.242    0.569\n   .TSC2              0.201    0.020   10.143    0.000    0.201    0.521\n   .TSC3              0.241    0.018   13.075    0.000    0.241    0.605\n   .TSC5              0.212    0.020   10.661    0.000    0.212    0.473\n   .TE1               0.195    0.018   11.019    0.000    0.195    0.378\n   .TE2               0.215    0.020   10.979    0.000    0.215    0.444\n   .TE3               0.177    0.017   10.315    0.000    0.177    0.379\n   .TE5               0.316    0.029   11.033    0.000    0.316    0.579\n   .EE1               0.261    0.026   10.042    0.000    0.261    0.454\n   .EE2               0.244    0.024   10.013    0.000    0.244    0.356\n   .EE3               0.258    0.026    9.770    0.000    0.258    0.382\n   .EE4               0.274    0.029    9.610    0.000    0.274    0.423\n   .DE1               0.248    0.023   10.614    0.000    0.248    0.558\n   .DE2               0.288    0.023   12.766    0.000    0.288    0.591\n   .DE3               0.241    0.029    8.396    0.000    0.241    0.455\n   .RPA1              0.181    0.022    8.246    0.000    0.181    0.279\n   .RPA2              0.167    0.021    7.812    0.000    0.167    0.271\n   .RPA3              0.237    0.042    5.590    0.000    0.237    0.379\n   .RPA4              0.359    0.040    9.004    0.000    0.359    0.655\n    TSC               1.000                               1.000    1.000\n    TE                1.000                               1.000    1.000\n    EE                1.000                               1.000    1.000\n    DE                1.000                               1.000    1.000\n    RPA               1.000                               1.000    1.000\n\n\n\n\n\nlibrary(parameters)\nmodel_parameters(CFA.results.train, standardize = \"refit\")\n\n# Loading\n\nLink        | Coefficient |   SE |       95% CI |     z |      p\n----------------------------------------------------------------\nTSC =~ TSC1 |        0.43 | 0.03 | [0.37, 0.49] | 14.61 | &lt; .001\nTSC =~ TSC2 |        0.43 | 0.03 | [0.37, 0.48] | 15.32 | &lt; .001\nTSC =~ TSC3 |        0.40 | 0.03 | [0.33, 0.46] | 11.91 | &lt; .001\nTSC =~ TSC5 |        0.49 | 0.03 | [0.43, 0.54] | 16.49 | &lt; .001\nTE =~ TE1   |        0.57 | 0.03 | [0.50, 0.63] | 16.51 | &lt; .001\nTE =~ TE2   |        0.52 | 0.03 | [0.47, 0.57] | 19.36 | &lt; .001\nTE =~ TE3   |        0.54 | 0.04 | [0.46, 0.61] | 13.88 | &lt; .001\nTE =~ TE5   |        0.48 | 0.04 | [0.40, 0.56] | 11.77 | &lt; .001\nEE =~ EE1   |        0.56 | 0.03 | [0.49, 0.63] | 16.05 | &lt; .001\nEE =~ EE2   |        0.66 | 0.03 | [0.60, 0.73] | 19.88 | &lt; .001\nEE =~ EE3   |        0.65 | 0.03 | [0.58, 0.71] | 18.49 | &lt; .001\nEE =~ EE4   |        0.61 | 0.03 | [0.54, 0.68] | 17.70 | &lt; .001\nDE =~ DE1   |        0.44 | 0.04 | [0.36, 0.53] | 10.59 | &lt; .001\nDE =~ DE2   |        0.45 | 0.04 | [0.37, 0.52] | 11.16 | &lt; .001\nDE =~ DE3   |        0.54 | 0.04 | [0.47, 0.61] | 14.75 | &lt; .001\nRPA =~ RPA1 |        0.68 | 0.04 | [0.61, 0.75] | 19.47 | &lt; .001\nRPA =~ RPA2 |        0.67 | 0.04 | [0.60, 0.74] | 17.95 | &lt; .001\nRPA =~ RPA3 |        0.62 | 0.04 | [0.54, 0.70] | 15.40 | &lt; .001\nRPA =~ RPA4 |        0.43 | 0.04 | [0.35, 0.52] | 10.06 | &lt; .001\n\n# Correlation\n\nLink       | Coefficient |   SE |       95% CI |     z |      p\n---------------------------------------------------------------\nTSC ~~ TE  |        0.66 | 0.04 | [0.57, 0.75] | 14.69 | &lt; .001\nTSC ~~ EE  |        0.77 | 0.04 | [0.70, 0.84] | 21.50 | &lt; .001\nTSC ~~ DE  |        0.56 | 0.06 | [0.45, 0.68] |  9.37 | &lt; .001\nTSC ~~ RPA |        0.69 | 0.04 | [0.61, 0.76] | 18.72 | &lt; .001\nTE ~~ EE   |        0.69 | 0.04 | [0.61, 0.77] | 16.82 | &lt; .001\nTE ~~ DE   |        0.50 | 0.06 | [0.38, 0.61] |  8.45 | &lt; .001\nTE ~~ RPA  |        0.65 | 0.04 | [0.56, 0.74] | 14.63 | &lt; .001\nEE ~~ DE   |        0.44 | 0.06 | [0.32, 0.56] |  7.27 | &lt; .001\nEE ~~ RPA  |        0.74 | 0.04 | [0.67, 0.82] | 19.59 | &lt; .001\nDE ~~ RPA  |        0.34 | 0.06 | [0.22, 0.46] |  5.74 | &lt; .001\n\n\n\n\n\n#install.packages(\"semPlot\") no sirve\n\n\n\n\n\n\n\n\n\nPara ver el ajuste del modelo cfa hay medidas globales y locales, respecto de las globales se tienen las siguientes metricas que evaluan el ajuste y sus repectivas regla :\n\nCFI &gt; 0.90 = buen ajuste.\nRMSEA &lt; 0.08 = buen ajuste.\nSRMR &lt; 0.08 = buen ajuste absoluto.\n\n\n\n\n\n# Resumen de los parámetros\n#inspect(CFA.results, \"std\")          # Cargas estandarizadas\n#inspect(CFA.results, \"cov.lv\")       # Covarianzas entre factores\n\n# Índices de ajuste\nsemTools::fitMeasures(CFA.results.train, c(\"cfi\", \"tli\", \"rmsea\", \"srmr\", \"aic\", \"bic\"))\n\n      cfi       tli     rmsea      srmr       aic       bic \n    0.946     0.935     0.058     0.044 14688.751 14962.260 \n\n# Fiabilidad compuesta y AVE\n#reliability(CFA.results.train)\n\n\n\n\nHay varias medias, pero las mas pertinente corresponde a analizar la diferencia entre la matriz de varianza covarianza entre model-implied y la muestral\n\nlibrary(lavaan)\n\n\n# Obtener matriz de correlaciones observadas\nr_observed &lt;- lavInspect(CFA.results.train, \"sampstat\")$cov\n\n# Obtener matriz de correlaciones reproducidas por el modelo\nr_model    &lt;-  lavInspect(CFA.results.train, \"implied\")$cov\n\n# Calcular matriz de residuos estandarizados\nresidual_matrix &lt;- r_observed - r_model\n\nprint(residual_matrix)\n\n       TSC1   TSC2   TSC3   TSC5    TE1    TE2    TE3    TE5    EE1    EE2\nTSC1  0.000                                                               \nTSC2 -0.012  0.000                                                        \nTSC3  0.007  0.012  0.000                                                 \nTSC5  0.007 -0.002 -0.010  0.000                                          \nTE1  -0.019  0.000 -0.009  0.010  0.000                                   \nTE2   0.025 -0.014 -0.031  0.021  0.011  0.000                            \nTE3   0.013 -0.010 -0.048 -0.005  0.003  0.008  0.000                     \nTE5   0.025  0.028  0.032  0.022 -0.012 -0.026 -0.005  0.000              \nEE1  -0.013  0.010  0.004 -0.016  0.042  0.044  0.001  0.072  0.000       \nEE2   0.004 -0.009  0.025 -0.003 -0.029 -0.050 -0.027  0.043 -0.002  0.000\nEE3  -0.013  0.015 -0.039  0.013 -0.021 -0.042 -0.006  0.081 -0.012 -0.001\nEE4  -0.002  0.002  0.000  0.013 -0.042  0.021  0.006  0.039 -0.017  0.017\nDE1  -0.011 -0.019  0.015  0.002  0.010 -0.026 -0.011  0.036 -0.010 -0.048\nDE2  -0.014  0.018  0.030 -0.011 -0.008 -0.025 -0.032  0.059  0.058  0.031\nDE3   0.000 -0.008  0.041 -0.021  0.023  0.006 -0.012  0.019  0.048 -0.015\nRPA1 -0.008  0.015 -0.034  0.011 -0.013 -0.022 -0.001  0.012  0.011 -0.018\nRPA2 -0.006 -0.008 -0.044  0.007 -0.021  0.004 -0.009  0.008  0.015 -0.016\nRPA3  0.041  0.016 -0.012  0.003 -0.006  0.010  0.017  0.034  0.035  0.008\nRPA4  0.020  0.000  0.003  0.031  0.001  0.027  0.031  0.039  0.042  0.035\n        EE3    EE4    DE1    DE2    DE3   RPA1   RPA2   RPA3   RPA4\nTSC1                                                               \nTSC2                                                               \nTSC3                                                               \nTSC5                                                               \nTE1                                                                \nTE2                                                                \nTE3                                                                \nTE5                                                                \nEE1                                                                \nEE2                                                                \nEE3   0.000                                                        \nEE4   0.010  0.000                                                 \nDE1  -0.042 -0.040  0.000                                          \nDE2   0.012  0.052  0.004  0.000                                   \nDE3  -0.022  0.012  0.002 -0.006  0.000                            \nRPA1  0.019 -0.041 -0.008 -0.006  0.002  0.000                     \nRPA2 -0.002 -0.053 -0.010 -0.025 -0.024  0.024  0.000              \nRPA3  0.022 -0.009  0.002  0.016  0.021 -0.009 -0.017  0.000       \nRPA4  0.031  0.053  0.006  0.056  0.074 -0.046 -0.011  0.052  0.000\n\nmax(residual_matrix)\n\n[1] 0.08051991\n\n\n\n\n\n\n\nsi el modelo ajusta bien tambien en el conjunto de datos de entrenamiento, puedes estar seguro que el modelo realmente captura bien la estructura factorial subyacente\n\n\nCFA.results.test &lt;- cfa(model        = CFA_model,\n                       data          = db_test,\n                       estimator     = \"MLR\",\n                       std.lv        = TRUE,\n                       meanstructure = TRUE)"
  },
  {
    "objectID": "Articles/2025-01-01_Factor_Analysis/analisis confirmatorio.html#carga-de-paquetes-datos",
    "href": "Articles/2025-01-01_Factor_Analysis/analisis confirmatorio.html#carga-de-paquetes-datos",
    "title": "Analisis factorial",
    "section": "",
    "text": "library(lavaan)\nlibrary(psych)\nlibrary(semTools)   #funcion para analisis de resultados,\nlibrary(tidyverse)\nlibrary(devtools)\nlibrary(readxl)\n\n#install.packages(\"skimr\")\nlibrary(skimr) # package\n\n#Improve L& f of tables\nlibrary(pander)\n\nlibrary(rio)\nlibrary(effectsize)\n\ndb_respuesta &lt;- read_excel(\"respuesta.xlsx\")"
  },
  {
    "objectID": "Articles/2025-01-01_Factor_Analysis/analisis confirmatorio.html#analisis-factorial-integrado",
    "href": "Articles/2025-01-01_Factor_Analysis/analisis confirmatorio.html#analisis-factorial-integrado",
    "title": "Analisis factorial",
    "section": "",
    "text": "Se emplea un estrategia en 3 pasos muchas veces empleada que integra el analisis factorial exploratorio y el analisis factorial confirmatorio (otras estrategias solo se emplea uno de los 2):\n\nExplorar la estructura factorial (Evaluacion de supuestos).\nConstruir el modelo factorial y evaluar el ajuste(estimacion EFA, CFA, criterios de informacion).\nEvaluar generabilidad (ajustar la muestra en los datos de prueba)\n\n\n\nLos resultados del modelo EFA, finalmente se validan a travez del modelo CFA, de modo tal que la evaluacion de supuestos de uno corresponde tambien la del otro\n\n\n\n\n\n\n\nskim(db_respuesta) %&gt;% \npander()\n\n\nTable continues below\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\nnumeric.mean\n\n\n\n\nnumeric\nTSC1\n0\n1\n3.653\n\n\nnumeric\nTSC2\n0\n1\n3.809\n\n\nnumeric\nTSC3\n0\n1\n3.732\n\n\nnumeric\nTSC4\n0\n1\n3.709\n\n\nnumeric\nTSC5\n0\n1\n3.822\n\n\nnumeric\nTE1\n0\n1\n4.061\n\n\nnumeric\nTE2\n0\n1\n4.043\n\n\nnumeric\nTE3\n0\n1\n4.121\n\n\nnumeric\nTE4\n0\n1\n4.105\n\n\nnumeric\nTE5\n0\n1\n3.902\n\n\nnumeric\nEE1\n0\n1\n3.812\n\n\nnumeric\nEE2\n0\n1\n3.727\n\n\nnumeric\nEE3\n0\n1\n3.878\n\n\nnumeric\nEE4\n0\n1\n3.687\n\n\nnumeric\nEE5\n0\n1\n3.987\n\n\nnumeric\nDE1\n0\n1\n3.925\n\n\nnumeric\nDE2\n0\n1\n3.596\n\n\nnumeric\nDE3\n0\n1\n3.817\n\n\nnumeric\nRPA1\n0\n1\n3.933\n\n\nnumeric\nRPA2\n0\n1\n3.941\n\n\nnumeric\nRPA3\n0\n1\n3.884\n\n\nnumeric\nRPA4\n0\n1\n3.869\n\n\nnumeric\nRPA5\n0\n1\n3.842\n\n\n\n\nTable continues below\n\n\n\n\n\n\n\n\n\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\n\n\n\n\n0.6849\n1\n3\n4\n4\n\n\n0.6389\n2\n3\n4\n4\n\n\n0.6396\n2\n3\n4\n4\n\n\n0.6734\n2\n3\n4\n4\n\n\n0.6522\n2\n3\n4\n4\n\n\n0.7122\n1\n4\n4\n5\n\n\n0.698\n2\n4\n4\n5\n\n\n0.7052\n1\n4\n4\n5\n\n\n0.6898\n1\n4\n4\n5\n\n\n0.7541\n1\n3\n4\n4\n\n\n0.7589\n1\n3\n4\n4\n\n\n0.8484\n1\n3\n4\n4\n\n\n0.828\n1\n3\n4\n4\n\n\n0.7959\n1\n3\n4\n4\n\n\n0.8105\n1\n3\n4\n5\n\n\n0.677\n1\n4\n4\n4\n\n\n0.682\n1\n3\n4\n4\n\n\n0.6984\n1\n3\n4\n4\n\n\n0.8343\n1\n3\n4\n5\n\n\n0.8049\n1\n4\n4\n4\n\n\n0.7886\n1\n3\n4\n4\n\n\n0.7649\n1\n3\n4\n4\n\n\n0.7858\n1\n3\n4\n4\n\n\n\n\n\n\n\n\n\n\nnumeric.p100\nnumeric.hist\n\n\n\n\n5\n▁▁▆▇▂\n\n\n5\n▁▃▁▇▂\n\n\n5\n▁▅▁▇▁\n\n\n5\n▁▅▁▇▂\n\n\n5\n▁▃▁▇▂\n\n\n5\n▁▁▂▇▃\n\n\n5\n▁▃▁▇▃\n\n\n5\n▁▁▂▇▅\n\n\n5\n▁▁▂▇▃\n\n\n5\n▁▁▃▇▃\n\n\n5\n▁▁▅▇▂\n\n\n5\n▁▁▆▇▃\n\n\n5\n▁▁▅▇▅\n\n\n5\n▁▁▇▇▃\n\n\n5\n▁▁▅▇▅\n\n\n5\n▁▁▂▇▂\n\n\n5\n▁▁▆▇▁\n\n\n5\n▁▁▅▇▂\n\n\n5\n▁▁▅▇▅\n\n\n5\n▁▁▃▇▃\n\n\n5\n▁▁▃▇▃\n\n\n5\n▁▁▃▇▃\n\n\n5\n▁▁▃▇▃\n\n\n\n\n\n\n\n\n\nRule of thumb &gt; 200 registros\n\n\n\nUn primer requerimiento antes de comenzar con el analisis factorial, es determinar el nivel de medicion;es decir, si son variables ordinales, cuantitativas, nominales, ya que la estructura de correlacion es la base del analisis factorial de modo tal que dependiendo de nivel de medicion sera necesario emplear una correlacion: pearson,tetracoricas,policoricas.\nVariables con similar rango y continuas\nPor defecto muchos paquetes en R asumen que son variables cuantitativas, variables obsersables con &gt;5 niveles.\n\n\n\n\n  db.longer_normal &lt;-   db_respuesta                                                                  |&gt; \n                        select(where(is.numeric))                                                     |&gt; \n                        pivot_longer(cols = everything(), names_to = \"Variable_obs\", values_to = \"Valor\") \n\n  db.longer_normal     |&gt; \n  ggplot(aes(x = Valor)) +\n  geom_histogram(bins = 30, fill = \"steelblue\", color = \"white\", alpha = 0.7) +\n  facet_wrap(~ Variable_obs, scales = \"free\") +  # Crear subgráficos por variable\n  theme_minimal() +\n  labs(title = \"Distribución de Variables Numéricas\", x = \"Variable Obs\", y = \"Frecuencia\")\n\n\n\n\n\n\n\n\nHay reglas of generales(rules of thumb), que establecen que ambos estadisticos no deben separar 3\n\n\ne1071skimrpyschpysch + pander\n\n\n\nlibrary(e1071)\n\ntbl.stats   &lt;-  db.longer_normal                                            |&gt;\n                group_by(Variable_obs)                                      |&gt;\n                summarise(\n                  skewness = e1071::skewness(Valor, na.rm = TRUE),\n                  kurtosis = e1071::kurtosis(Valor, na.rm = TRUE)\n                )\nlibrary(gt)  \n  \n  \n  tbl.stats                             |&gt;\n  gt()                                  |&gt;\n  data_color(\n    columns = vars(skewness),\n    colors = scales::col_bin(\n      bins = c(-Inf, 0, Inf),\n      palette = c(\"coral\", \"aliceblue\")\n    )\n  )                                     |&gt;\n  data_color(\n    columns = vars(kurtosis),\n    colors = scales::col_bin(\n      bins = c(-Inf, 0, Inf),\n      palette = c(\"coral\", \"aliceblue\")\n    )\n  )                                     |&gt;\n  tab_header(\n    title = \"Asimetría y Curtosis por Variable\"\n  )\n\n\n\n\n\n  \n    \n      Asimetría y Curtosis por Variable\n    \n    \n    \n      Variable_obs\n      skewness\n      kurtosis\n    \n  \n  \n    DE1\n-0.52605215\n1.24559696\n    DE2\n-0.21897007\n0.63925394\n    DE3\n-0.13677255\n0.01245932\n    EE1\n-0.34581133\n0.23143832\n    EE2\n-0.36964012\n0.12256023\n    EE3\n-0.31280354\n-0.39901657\n    EE4\n-0.03410964\n-0.40742547\n    EE5\n-0.42732927\n-0.26944283\n    RPA1\n-0.59305403\n0.50420600\n    RPA2\n-0.78552074\n1.22272528\n    RPA3\n-0.58790017\n0.74890573\n    RPA4\n-0.47810685\n0.33329229\n    RPA5\n-0.53468885\n0.67055640\n    TE1\n-0.46692932\n0.37701716\n    TE2\n-0.22002197\n-0.44900737\n    TE3\n-0.72100623\n1.60041195\n    TE4\n-0.47367810\n0.50711668\n    TE5\n-0.41165968\n0.16474956\n    TSC1\n-0.09242437\n0.06296766\n    TSC2\n-0.07293765\n-0.14123339\n    TSC3\n-0.16580865\n-0.01842834\n    TSC4\n-0.02578558\n-0.25212689\n    TSC5\n-0.09955242\n-0.12934441\n  \n  \n  \n\n\n\n\n\n\n\nskim(db_respuesta)\n\n\nData summary\n\n\nName\ndb_respuesta\n\n\nNumber of rows\n876\n\n\nNumber of columns\n23\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n23\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nTSC1\n0\n1\n3.65\n0.68\n1\n3\n4\n4\n5\n▁▁▆▇▂\n\n\nTSC2\n0\n1\n3.81\n0.64\n2\n3\n4\n4\n5\n▁▃▁▇▂\n\n\nTSC3\n0\n1\n3.73\n0.64\n2\n3\n4\n4\n5\n▁▅▁▇▁\n\n\nTSC4\n0\n1\n3.71\n0.67\n2\n3\n4\n4\n5\n▁▅▁▇▂\n\n\nTSC5\n0\n1\n3.82\n0.65\n2\n3\n4\n4\n5\n▁▃▁▇▂\n\n\nTE1\n0\n1\n4.06\n0.71\n1\n4\n4\n5\n5\n▁▁▂▇▃\n\n\nTE2\n0\n1\n4.04\n0.70\n2\n4\n4\n5\n5\n▁▃▁▇▃\n\n\nTE3\n0\n1\n4.12\n0.71\n1\n4\n4\n5\n5\n▁▁▂▇▅\n\n\nTE4\n0\n1\n4.11\n0.69\n1\n4\n4\n5\n5\n▁▁▂▇▃\n\n\nTE5\n0\n1\n3.90\n0.75\n1\n3\n4\n4\n5\n▁▁▃▇▃\n\n\nEE1\n0\n1\n3.81\n0.76\n1\n3\n4\n4\n5\n▁▁▅▇▂\n\n\nEE2\n0\n1\n3.73\n0.85\n1\n3\n4\n4\n5\n▁▁▆▇▃\n\n\nEE3\n0\n1\n3.88\n0.83\n1\n3\n4\n4\n5\n▁▁▅▇▅\n\n\nEE4\n0\n1\n3.69\n0.80\n1\n3\n4\n4\n5\n▁▁▇▇▃\n\n\nEE5\n0\n1\n3.99\n0.81\n1\n3\n4\n5\n5\n▁▁▅▇▅\n\n\nDE1\n0\n1\n3.92\n0.68\n1\n4\n4\n4\n5\n▁▁▂▇▂\n\n\nDE2\n0\n1\n3.60\n0.68\n1\n3\n4\n4\n5\n▁▁▆▇▁\n\n\nDE3\n0\n1\n3.82\n0.70\n1\n3\n4\n4\n5\n▁▁▅▇▂\n\n\nRPA1\n0\n1\n3.93\n0.83\n1\n3\n4\n5\n5\n▁▁▅▇▅\n\n\nRPA2\n0\n1\n3.94\n0.80\n1\n4\n4\n4\n5\n▁▁▃▇▃\n\n\nRPA3\n0\n1\n3.88\n0.79\n1\n3\n4\n4\n5\n▁▁▃▇▃\n\n\nRPA4\n0\n1\n3.87\n0.76\n1\n3\n4\n4\n5\n▁▁▃▇▃\n\n\nRPA5\n0\n1\n3.84\n0.79\n1\n3\n4\n4\n5\n▁▁▃▇▃\n\n\n\n\n\n\n\n\npsych::describe(db_respuesta)\n\n     vars   n mean   sd median trimmed  mad min max range  skew kurtosis   se\nTSC1    1 876 3.65 0.68      4    3.62 0.00   1   5     4 -0.09     0.06 0.02\nTSC2    2 876 3.81 0.64      4    3.78 0.00   2   5     3 -0.07    -0.14 0.02\nTSC3    3 876 3.73 0.64      4    3.71 0.00   2   5     3 -0.17    -0.02 0.02\nTSC4    4 876 3.71 0.67      4    3.67 0.00   2   5     3 -0.03    -0.25 0.02\nTSC5    5 876 3.82 0.65      4    3.79 0.00   2   5     3 -0.10    -0.13 0.02\nTE1     6 876 4.06 0.71      4    4.10 0.00   1   5     4 -0.47     0.38 0.02\nTE2     7 876 4.04 0.70      4    4.07 0.00   2   5     3 -0.22    -0.45 0.02\nTE3     8 876 4.12 0.71      4    4.17 0.00   1   5     4 -0.72     1.60 0.02\nTE4     9 876 4.11 0.69      4    4.15 0.00   1   5     4 -0.47     0.51 0.02\nTE5    10 876 3.90 0.75      4    3.92 0.00   1   5     4 -0.41     0.16 0.03\nEE1    11 876 3.81 0.76      4    3.81 0.00   1   5     4 -0.35     0.23 0.03\nEE2    12 876 3.73 0.85      4    3.75 1.48   1   5     4 -0.37     0.12 0.03\nEE3    13 876 3.88 0.83      4    3.91 1.48   1   5     4 -0.31    -0.40 0.03\nEE4    14 876 3.69 0.80      4    3.67 1.48   1   5     4 -0.03    -0.41 0.03\nEE5    15 876 3.99 0.81      4    4.03 1.48   1   5     4 -0.43    -0.27 0.03\nDE1    16 876 3.92 0.68      4    3.93 0.00   1   5     4 -0.53     1.25 0.02\nDE2    17 876 3.60 0.68      4    3.58 1.48   1   5     4 -0.22     0.64 0.02\nDE3    18 876 3.82 0.70      4    3.79 0.00   1   5     4 -0.14     0.01 0.02\nRPA1   19 876 3.93 0.83      4    3.97 1.48   1   5     4 -0.59     0.50 0.03\nRPA2   20 876 3.94 0.80      4    3.99 0.00   1   5     4 -0.79     1.22 0.03\nRPA3   21 876 3.88 0.79      4    3.91 0.00   1   5     4 -0.59     0.75 0.03\nRPA4   22 876 3.87 0.76      4    3.89 0.00   1   5     4 -0.48     0.33 0.03\nRPA5   23 876 3.84 0.79      4    3.86 0.00   1   5     4 -0.53     0.67 0.03\n\n\n\n\n\npsych::describe(db_respuesta) %&gt;% \npander()\n\n\nTable continues below\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\n\n\n\n\nTSC1\n1\n876\n3.653\n0.6849\n4\n3.62\n0\n1\n5\n\n\nTSC2\n2\n876\n3.809\n0.6389\n4\n3.776\n0\n2\n5\n\n\nTSC3\n3\n876\n3.732\n0.6396\n4\n3.708\n0\n2\n5\n\n\nTSC4\n4\n876\n3.709\n0.6734\n4\n3.665\n0\n2\n5\n\n\nTSC5\n5\n876\n3.822\n0.6522\n4\n3.795\n0\n2\n5\n\n\nTE1\n6\n876\n4.061\n0.7122\n4\n4.098\n0\n1\n5\n\n\nTE2\n7\n876\n4.043\n0.698\n4\n4.066\n0\n2\n5\n\n\nTE3\n8\n876\n4.121\n0.7052\n4\n4.174\n0\n1\n5\n\n\nTE4\n9\n876\n4.105\n0.6898\n4\n4.148\n0\n1\n5\n\n\nTE5\n10\n876\n3.902\n0.7541\n4\n3.923\n0\n1\n5\n\n\nEE1\n11\n876\n3.812\n0.7589\n4\n3.815\n0\n1\n5\n\n\nEE2\n12\n876\n3.727\n0.8484\n4\n3.748\n1.483\n1\n5\n\n\nEE3\n13\n876\n3.878\n0.828\n4\n3.906\n1.483\n1\n5\n\n\nEE4\n14\n876\n3.687\n0.7959\n4\n3.672\n1.483\n1\n5\n\n\nEE5\n15\n876\n3.987\n0.8105\n4\n4.028\n1.483\n1\n5\n\n\nDE1\n16\n876\n3.925\n0.677\n4\n3.934\n0\n1\n5\n\n\nDE2\n17\n876\n3.596\n0.682\n4\n3.577\n1.483\n1\n5\n\n\nDE3\n18\n876\n3.817\n0.6984\n4\n3.795\n0\n1\n5\n\n\nRPA1\n19\n876\n3.933\n0.8343\n4\n3.974\n1.483\n1\n5\n\n\nRPA2\n20\n876\n3.941\n0.8049\n4\n3.991\n0\n1\n5\n\n\nRPA3\n21\n876\n3.884\n0.7886\n4\n3.913\n0\n1\n5\n\n\nRPA4\n22\n876\n3.869\n0.7649\n4\n3.893\n0\n1\n5\n\n\nRPA5\n23\n876\n3.842\n0.7858\n4\n3.863\n0\n1\n5\n\n\n\n\n\n\n\n\n\n\n\n\n\n \nrange\nskew\nkurtosis\nse\n\n\n\n\nTSC1\n4\n-0.09242\n0.06297\n0.02314\n\n\nTSC2\n3\n-0.07294\n-0.1412\n0.02159\n\n\nTSC3\n3\n-0.1658\n-0.01843\n0.02161\n\n\nTSC4\n3\n-0.02579\n-0.2521\n0.02275\n\n\nTSC5\n3\n-0.09955\n-0.1293\n0.02204\n\n\nTE1\n4\n-0.4669\n0.377\n0.02406\n\n\nTE2\n3\n-0.22\n-0.449\n0.02358\n\n\nTE3\n4\n-0.721\n1.6\n0.02383\n\n\nTE4\n4\n-0.4737\n0.5071\n0.02331\n\n\nTE5\n4\n-0.4117\n0.1647\n0.02548\n\n\nEE1\n4\n-0.3458\n0.2314\n0.02564\n\n\nEE2\n4\n-0.3696\n0.1226\n0.02866\n\n\nEE3\n4\n-0.3128\n-0.399\n0.02798\n\n\nEE4\n4\n-0.03411\n-0.4074\n0.02689\n\n\nEE5\n4\n-0.4273\n-0.2694\n0.02739\n\n\nDE1\n4\n-0.5261\n1.246\n0.02287\n\n\nDE2\n4\n-0.219\n0.6393\n0.02304\n\n\nDE3\n4\n-0.1368\n0.01246\n0.0236\n\n\nRPA1\n4\n-0.5931\n0.5042\n0.02819\n\n\nRPA2\n4\n-0.7855\n1.223\n0.0272\n\n\nRPA3\n4\n-0.5879\n0.7489\n0.02664\n\n\nRPA4\n4\n-0.4781\n0.3333\n0.02584\n\n\nRPA5\n4\n-0.5347\n0.6706\n0.02655\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEs necesario validar que existe suficiente correlacion entre las variables observadas,sino no tiene caso si quiera plantear la existencia de factores, para ellos se puede calcular el test de Bartlett, el cual testea :\nho : Matriz de correlacion es una matriz identidad (0’s fuera de la diagonal) h1 : no lo es\n\n# nobmres de columnas\nvarnames                 &lt;- colnames(db_respuesta)\n\n## Matriz de correlacion \nmtr.corr_resp            &lt;- cor(db_respuesta[,varnames])\n\nmtr.corr_bart_result     &lt;-psych::cortest.bartlett(R=mtr.corr_resp,n = nrow(db_respuesta))\n\nmtr.corr_bart_result$p.value&lt;0.05  # Mucho cuidado con n, dependiende de su dimensiodad varia de significativo o no \n\n[1] TRUE\n\n\n\n\n\n\nPara ello requerimos el kmo que se puede entender como la propocion de la varianza total, explicada por los factores\nEl estado kmo , en particular se establece los siguientes umbrales :\n\n\n\n\nKMO\nInterpretación\n\n\n\n\n≥ 0.90\nExcelente\n\n\n0.80–0.89\nMuy bueno\n\n\n0.70–0.79\nAceptable\n\n\n0.60–0.69\nMediocre\n\n\n0.50–0.59\nPobre\n\n\n&lt; 0.50\nInaceptable\n\n\n\n\nKMO_result &lt;- KMO(db_respuesta)\n\n\n\n\n\nSe recomienda tener una muestra para probar los resultados del modelo factorial, al respecto :\n\npara calcular el modelo factorial : n(tamaño muestra) = 5*numero de parametros\nconsiderar tener una muestra de testeo/ evaluacion para extrapolar/generalizar resultados : 2*n\n\n\n# tamaño de muestra mitad\nmitad_muestra     &lt;- nrow(db_respuesta)/2\n\nset.seed(19)\n\n## Vector para marcar datos\nvc_marca          &lt;- sample(c(rep(\"model.building\",mitad_muestra),\n                         rep(\"holdout\",mitad_muestra)\n                         ))\n\n## Dividir el vecto x, segun grupos definidos en el vector y \n\ndb_model.build_list   &lt;- split(db_respuesta,vc_marca)\n\n\ndb_train              &lt;-db_model.build_list$model.building\ndb_test               &lt;-db_model.build_list$holdout"
  },
  {
    "objectID": "Articles/2025-01-01_Factor_Analysis/analisis confirmatorio.html#efa-analisis-factorial-exploratorio",
    "href": "Articles/2025-01-01_Factor_Analysis/analisis confirmatorio.html#efa-analisis-factorial-exploratorio",
    "title": "Analisis factorial",
    "section": "",
    "text": "Parte de un supuesto clave: Existen los factores, pero no se saben cuantos son ni que variables observables lo componen\nPuede ser realizada con 2 metodos, que no necesariamente se menoscaban, de hecho se complementan :\n\n\nAnalisis paralelo : Para detectar el numero de factores(ejm constructos)\nAnalisis de ajuste : Para detectar el ajuste del modelo con el numero de factores decidido en a) se puede calcular criterios de informacion ejm BIC\n\n\nparalel_results &lt;- fa.parallel(x =db_train,fa=\"fa\")\n\n\n\n\nParallel analysis suggests that the number of factors =  5  and the number of components =  NA \n\n# cargas factoriales : representan las correlaciones entre las variables obser y el factor latente, si los datos \n# estuvieran estadarizados\n\nvalue.factor       &lt;- paralel_results$fa.values    # factorial loadins\nvalue.factor.simil &lt;- paralel_results$fa.sim\nn.factor           &lt;- 1:length(value.factor)       # numero de factores\n\nscreen.data  &lt;- tibble(loading       = value.factor, \n                       loading_simul = value.factor.simil,\n                       n_factor      = n.factor)\n\n\nScreen plotAnalisis paralelo\n\n\n\n# Idealmente se espera cargas superior a 0.5 (gpt)\nscreen.data                                                     |&gt;\nggplot(aes(n.factor,value.factor))                              +\ngeom_line(size = 1, \n            color = \"lightblue\")                                +\ngeom_hline(yintercept = 0.5, linetype = \"dashed\",color = \"red\") +\ngeom_point(color=\"cornflowerblue\", \n             size = 2, \n             alpha=.8)                                          +\ntheme_bw()                                                      +\nxlab(\"Numero de factores\")                                      +\nylab(\"carga factorial\")\n\n\n\n\n\n\n\n## Analisis paralelo\nscreen.data                        |&gt;\nggplot() +\ngeom_line(aes(n.factor,value.factor),\n            size = 1, \n            color = \"lightblue\")   +\ngeom_point(aes(n.factor,value.factor),\n             color=\"cornflowerblue\", \n             size = 2, \n             alpha=.8)             +\ngeom_line(aes(n.factor,value.factor.simil),\n            size = 1, \n            color = \"darkred\")   +\ngeom_point(aes(n.factor,value.factor.simil),\n             color=\"darkred\", \n             size = 2, \n             alpha=.8)             +\nggtitle(\"Analisis paralelo\")       +\ntheme_bw()                         +\nxlab(\"Numero de factores\")         +\nylab(\"carga factorial\")\n\n\n\n\n\n\n\nEsta metodologia solo dice un numero plausible de factores, mas no el numero final(lo mejor es considerar el rango que lo limita) , el numero final de factores debe ser decidido finalmente en base a : la interpretabilidad y el ajuste\n\n\nLa varianza de los items tiene 3 componentes : varianza compartida, especifica, y varianza del error - comunalidad,especificidad y varianza residual .\nPara estimar un modelo asi se debe especificar: * El numero de factores * El metodo de estimacion * la rotacion de la matriz\nAsi para estimar el modelo hay 3 principales tecnicas: * ML * PCA * PAF\n\nresults.efa &lt;- lavaan::efa(data      = db_train,\n                           nfactors  = 4:6,     # - el numero otorgado por el analisis paralelo otorga\n                                                #   el rango, el cual seria le numero de factores adecuado.\n                           rotation  = \"geomin\",# - se escoge este metodo de rotacion xq permite que haya correlacion\n                                                # entre factores y tiene sentido pues se habla de constructos educativos\n                           estimator =\"MLR\",    # - Preferible metodo de estimacion al ML, por posibles violaciones del\n                                                # supuesto de normalidad\n                           meanstructure = TRUE)\n\n\n\nDeacuerdo la metricas de los criterios de criterios de informacion queda claro que el mejor terminar ajustando es el modelo de 5 factores\n\nsort(lavaan::fitmeasures(results.efa)[\"bic\",])\n\nnfactors = 5 nfactors = 4 nfactors = 6 \n    18142.38     18167.49     18189.29 \n\n\n\n\n\nel paquete lavaan te otorga las cargas estandarizadas de modo tal que representarian la correlacion entre la variable observada y el factor\n\nresults.efa$nf5\n\n\n         f1      f2      f3      f4      f5 \nTSC1  0.584*                       *      .*\nTSC2  0.487*                       *      .*\nTSC3  0.637*                      .*       *\nTSC4  0.578*      .*              .*       *\nTSC5  0.547*                              . \nTE1           0.728*              .         \nTE2       .   0.672*                        \nTE3           0.708*      .                 \nTE4           0.651*              .*        \nTE5           0.337*      .*      .*        \nEE1               .   0.469*      .         \nEE2       .*          0.689*                \nEE3                   0.768*                \nEE4       .*          0.732*              . \nEE5               .   0.479*      .*        \nDE1                  -0.353*  0.744*      . \nDE2               .*          0.821*        \nDE3                       .*  0.755*        \nRPA1                                  0.851*\nRPA2                                  0.906*\nRPA3                                  0.624*\nRPA4                      .       .   0.350*\nRPA5                      .       .   0.338*\n\n\n\n\n\n\n\ndb_train_sumas &lt;- db_train %&gt;%\n                mutate(\n                  # Aplicamos across con una expresión lógica para cada grupo\n                  TSC = rowSums(across(matches(\"^TSC\\\\d+\")), na.rm = TRUE),\n                  EE  = rowSums(across(matches(\"^TE\\\\d+\")),  na.rm = TRUE),\n                  TE  = rowSums(across(matches(\"^EE\\\\d+\")),  na.rm = TRUE),\n                  DE  = rowSums(across(matches(\"^DE\\\\d+\")),  na.rm = TRUE),\n                  RPA = rowSums(across(matches(\"^RPA\\\\d+\")), na.rm = TRUE)\n                )\n\n\n#install.packages(\"BayesFM\")\nlibrary(\"BayesFM\")\n\n# specify model\nmodel &lt;- c(                                        # X1 covariate in all equations\n            paste0('TSC',' ~ ','TSC',1:5),         # X2 covariate for Y1-Y5 only\n            paste0('TE',' ~ ','TE',6:10),          # X3 covariate for Y6-Y10 only\n            paste0('EE',' ~ ','EE',11:15))         # X4 covariate for Y11-Y15 only\n\nmodel &lt;- lapply(model, as.formula) # make list of formulas\n\n# run MCMC sampler, post process and summarize\nmcmc &lt;- befa(model, data = db_train, Kmax = 5, iter = 1000)"
  },
  {
    "objectID": "Articles/2025-01-01_Factor_Analysis/analisis confirmatorio.html#cfa-analisis-factorial-integrado",
    "href": "Articles/2025-01-01_Factor_Analysis/analisis confirmatorio.html#cfa-analisis-factorial-integrado",
    "title": "Analisis factorial",
    "section": "",
    "text": "Este analisis pone a prueba la estructura establecida, para ello usamos la sintaxis lavaan de modelo, tal que asi:\n\nCFA_model &lt;- '\n  # Regressing items on factors,ejm el componente TSC es medido por TSC1 + TSC2 + TSC3 + TSC5\n  TSC =~ TSC1 + TSC2 + TSC3 + TSC5\n  TE  =~ TE1  + TE2  + TE3  + TE5\n  EE  =~ EE1  + EE2  + EE3  + EE4\n  DE  =~ DE1  + DE2  + DE3\n  RPA =~ RPA1 + RPA2 + RPA3 + RPA4\n\n  # Se permite correlacion entre factores\n  TSC ~~ TE\n  TSC ~~ EE\n  TSC ~~ DE\n  TSC ~~ RPA\n\n  TE ~~ EE\n  TE ~~ DE\n  TE ~~ RPA\n\n  EE ~~ DE\n  EE ~~ RPA\n\n  DE ~~ RPA\n'\n\nCFA.results.train &lt;- cfa( model         = CFA_model,\n                         data          = db_train,\n                         estimator     = \"MLR\",\n                         std.lv        = TRUE,\n                         meanstructure = TRUE)\n\n\n\n\nstd.all Corresponde a las cargas factoriales de cada variable observable en cada contructo\n\n\nA) DefaultB) Parameters packageC) Grafico del analisis confirmatorio\n\n\n\n# referenica : https://www.youtube.com/watch?v=GO5lCfHiFzE***\n\nsummary(CFA.results.train,standardized=T,fit.measures=T)\n\nlavaan 0.6-19 ended normally after 35 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        67\n\n  Number of observations                           438\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               352.092     318.841\n  Degrees of freedom                               142         142\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  1.104\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              4069.230    3549.780\n  Degrees of freedom                               171         171\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.146\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.946       0.948\n  Tucker-Lewis Index (TLI)                       0.935       0.937\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.950\n  Robust Tucker-Lewis Index (TLI)                            0.939\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -7277.376   -7277.376\n  Scaling correction factor                                  1.177\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -7101.330   -7101.330\n  Scaling correction factor                                  1.128\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               14688.751   14688.751\n  Bayesian (BIC)                             14962.260   14962.260\n  Sample-size adjusted Bayesian (SABIC)      14749.636   14749.636\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.058       0.053\n  90 Percent confidence interval - lower         0.051       0.046\n  90 Percent confidence interval - upper         0.066       0.061\n  P-value H_0: RMSEA &lt;= 0.050                    0.040       0.224\n  P-value H_0: RMSEA &gt;= 0.080                    0.000       0.000\n                                                                  \n  Robust RMSEA                                               0.056\n  90 Percent confidence interval - lower                     0.048\n  90 Percent confidence interval - upper                     0.064\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.110\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.044       0.044\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  TSC =~                                                                \n    TSC1              0.428    0.029   14.606    0.000    0.428    0.657\n    TSC2              0.430    0.028   15.320    0.000    0.430    0.692\n    TSC3              0.396    0.033   11.909    0.000    0.396    0.628\n    TSC5              0.486    0.029   16.492    0.000    0.486    0.726\n  TE =~                                                                 \n    TE1               0.567    0.034   16.507    0.000    0.567    0.789\n    TE2               0.519    0.027   19.357    0.000    0.519    0.745\n    TE3               0.538    0.039   13.883    0.000    0.538    0.788\n    TE5               0.479    0.041   11.774    0.000    0.479    0.649\n  EE =~                                                                 \n    EE1               0.560    0.035   16.046    0.000    0.560    0.739\n    EE2               0.664    0.033   19.879    0.000    0.664    0.802\n    EE3               0.645    0.035   18.492    0.000    0.645    0.786\n    EE4               0.612    0.035   17.698    0.000    0.612    0.760\n  DE =~                                                                 \n    DE1               0.444    0.042   10.595    0.000    0.444    0.665\n    DE2               0.446    0.040   11.159    0.000    0.446    0.640\n    DE3               0.537    0.036   14.749    0.000    0.537    0.738\n  RPA =~                                                                \n    RPA1              0.683    0.035   19.474    0.000    0.683    0.849\n    RPA2              0.670    0.037   17.951    0.000    0.670    0.854\n    RPA3              0.623    0.040   15.403    0.000    0.623    0.788\n    RPA4              0.435    0.043   10.064    0.000    0.435    0.587\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  TSC ~~                                                                \n    TE                0.659    0.045   14.688    0.000    0.659    0.659\n    EE                0.766    0.036   21.496    0.000    0.766    0.766\n    DE                0.564    0.060    9.374    0.000    0.564    0.564\n    RPA               0.686    0.037   18.725    0.000    0.686    0.686\n  TE ~~                                                                 \n    EE                0.689    0.041   16.816    0.000    0.689    0.689\n    DE                0.496    0.059    8.452    0.000    0.496    0.496\n    RPA               0.650    0.044   14.633    0.000    0.650    0.650\n  EE ~~                                                                 \n    DE                0.440    0.061    7.266    0.000    0.440    0.440\n    RPA               0.745    0.038   19.593    0.000    0.745    0.745\n  DE ~~                                                                 \n    RPA               0.340    0.059    5.742    0.000    0.340    0.340\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .TSC1              3.662    0.031  117.614    0.000    3.662    5.620\n   .TSC2              3.813    0.030  128.595    0.000    3.813    6.145\n   .TSC3              3.719    0.030  123.332    0.000    3.719    5.893\n   .TSC5              3.861    0.032  120.631    0.000    3.861    5.764\n   .TE1               4.064    0.034  118.349    0.000    4.064    5.655\n   .TE2               4.048    0.033  121.772    0.000    4.048    5.818\n   .TE3               4.103    0.033  125.699    0.000    4.103    6.006\n   .TE5               3.934    0.035  111.433    0.000    3.934    5.324\n   .EE1               3.808    0.036  105.097    0.000    3.808    5.022\n   .EE2               3.751    0.040   94.879    0.000    3.751    4.533\n   .EE3               3.884    0.039   99.026    0.000    3.884    4.732\n   .EE4               3.710    0.038   96.396    0.000    3.710    4.606\n   .DE1               3.934    0.032  123.361    0.000    3.934    5.894\n   .DE2               3.600    0.033  108.034    0.000    3.600    5.162\n   .DE3               3.808    0.035  109.535    0.000    3.808    5.234\n   .RPA1              3.947    0.038  102.635    0.000    3.947    4.904\n   .RPA2              3.963    0.037  105.764    0.000    3.963    5.054\n   .RPA3              3.893    0.038  103.011    0.000    3.893    4.922\n   .RPA4              3.904    0.035  110.386    0.000    3.904    5.274\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .TSC1              0.242    0.019   12.538    0.000    0.242    0.569\n   .TSC2              0.201    0.020   10.143    0.000    0.201    0.521\n   .TSC3              0.241    0.018   13.075    0.000    0.241    0.605\n   .TSC5              0.212    0.020   10.661    0.000    0.212    0.473\n   .TE1               0.195    0.018   11.019    0.000    0.195    0.378\n   .TE2               0.215    0.020   10.979    0.000    0.215    0.444\n   .TE3               0.177    0.017   10.315    0.000    0.177    0.379\n   .TE5               0.316    0.029   11.033    0.000    0.316    0.579\n   .EE1               0.261    0.026   10.042    0.000    0.261    0.454\n   .EE2               0.244    0.024   10.013    0.000    0.244    0.356\n   .EE3               0.258    0.026    9.770    0.000    0.258    0.382\n   .EE4               0.274    0.029    9.610    0.000    0.274    0.423\n   .DE1               0.248    0.023   10.614    0.000    0.248    0.558\n   .DE2               0.288    0.023   12.766    0.000    0.288    0.591\n   .DE3               0.241    0.029    8.396    0.000    0.241    0.455\n   .RPA1              0.181    0.022    8.246    0.000    0.181    0.279\n   .RPA2              0.167    0.021    7.812    0.000    0.167    0.271\n   .RPA3              0.237    0.042    5.590    0.000    0.237    0.379\n   .RPA4              0.359    0.040    9.004    0.000    0.359    0.655\n    TSC               1.000                               1.000    1.000\n    TE                1.000                               1.000    1.000\n    EE                1.000                               1.000    1.000\n    DE                1.000                               1.000    1.000\n    RPA               1.000                               1.000    1.000\n\n\n\n\n\nlibrary(parameters)\nmodel_parameters(CFA.results.train, standardize = \"refit\")\n\n# Loading\n\nLink        | Coefficient |   SE |       95% CI |     z |      p\n----------------------------------------------------------------\nTSC =~ TSC1 |        0.43 | 0.03 | [0.37, 0.49] | 14.61 | &lt; .001\nTSC =~ TSC2 |        0.43 | 0.03 | [0.37, 0.48] | 15.32 | &lt; .001\nTSC =~ TSC3 |        0.40 | 0.03 | [0.33, 0.46] | 11.91 | &lt; .001\nTSC =~ TSC5 |        0.49 | 0.03 | [0.43, 0.54] | 16.49 | &lt; .001\nTE =~ TE1   |        0.57 | 0.03 | [0.50, 0.63] | 16.51 | &lt; .001\nTE =~ TE2   |        0.52 | 0.03 | [0.47, 0.57] | 19.36 | &lt; .001\nTE =~ TE3   |        0.54 | 0.04 | [0.46, 0.61] | 13.88 | &lt; .001\nTE =~ TE5   |        0.48 | 0.04 | [0.40, 0.56] | 11.77 | &lt; .001\nEE =~ EE1   |        0.56 | 0.03 | [0.49, 0.63] | 16.05 | &lt; .001\nEE =~ EE2   |        0.66 | 0.03 | [0.60, 0.73] | 19.88 | &lt; .001\nEE =~ EE3   |        0.65 | 0.03 | [0.58, 0.71] | 18.49 | &lt; .001\nEE =~ EE4   |        0.61 | 0.03 | [0.54, 0.68] | 17.70 | &lt; .001\nDE =~ DE1   |        0.44 | 0.04 | [0.36, 0.53] | 10.59 | &lt; .001\nDE =~ DE2   |        0.45 | 0.04 | [0.37, 0.52] | 11.16 | &lt; .001\nDE =~ DE3   |        0.54 | 0.04 | [0.47, 0.61] | 14.75 | &lt; .001\nRPA =~ RPA1 |        0.68 | 0.04 | [0.61, 0.75] | 19.47 | &lt; .001\nRPA =~ RPA2 |        0.67 | 0.04 | [0.60, 0.74] | 17.95 | &lt; .001\nRPA =~ RPA3 |        0.62 | 0.04 | [0.54, 0.70] | 15.40 | &lt; .001\nRPA =~ RPA4 |        0.43 | 0.04 | [0.35, 0.52] | 10.06 | &lt; .001\n\n# Correlation\n\nLink       | Coefficient |   SE |       95% CI |     z |      p\n---------------------------------------------------------------\nTSC ~~ TE  |        0.66 | 0.04 | [0.57, 0.75] | 14.69 | &lt; .001\nTSC ~~ EE  |        0.77 | 0.04 | [0.70, 0.84] | 21.50 | &lt; .001\nTSC ~~ DE  |        0.56 | 0.06 | [0.45, 0.68] |  9.37 | &lt; .001\nTSC ~~ RPA |        0.69 | 0.04 | [0.61, 0.76] | 18.72 | &lt; .001\nTE ~~ EE   |        0.69 | 0.04 | [0.61, 0.77] | 16.82 | &lt; .001\nTE ~~ DE   |        0.50 | 0.06 | [0.38, 0.61] |  8.45 | &lt; .001\nTE ~~ RPA  |        0.65 | 0.04 | [0.56, 0.74] | 14.63 | &lt; .001\nEE ~~ DE   |        0.44 | 0.06 | [0.32, 0.56] |  7.27 | &lt; .001\nEE ~~ RPA  |        0.74 | 0.04 | [0.67, 0.82] | 19.59 | &lt; .001\nDE ~~ RPA  |        0.34 | 0.06 | [0.22, 0.46] |  5.74 | &lt; .001\n\n\n\n\n\n#install.packages(\"semPlot\") no sirve\n\n\n\n\n\n\n\n\n\nPara ver el ajuste del modelo cfa hay medidas globales y locales, respecto de las globales se tienen las siguientes metricas que evaluan el ajuste y sus repectivas regla :\n\nCFI &gt; 0.90 = buen ajuste.\nRMSEA &lt; 0.08 = buen ajuste.\nSRMR &lt; 0.08 = buen ajuste absoluto.\n\n\n\n\n\n# Resumen de los parámetros\n#inspect(CFA.results, \"std\")          # Cargas estandarizadas\n#inspect(CFA.results, \"cov.lv\")       # Covarianzas entre factores\n\n# Índices de ajuste\nsemTools::fitMeasures(CFA.results.train, c(\"cfi\", \"tli\", \"rmsea\", \"srmr\", \"aic\", \"bic\"))\n\n      cfi       tli     rmsea      srmr       aic       bic \n    0.946     0.935     0.058     0.044 14688.751 14962.260 \n\n# Fiabilidad compuesta y AVE\n#reliability(CFA.results.train)\n\n\n\n\nHay varias medias, pero las mas pertinente corresponde a analizar la diferencia entre la matriz de varianza covarianza entre model-implied y la muestral\n\nlibrary(lavaan)\n\n\n# Obtener matriz de correlaciones observadas\nr_observed &lt;- lavInspect(CFA.results.train, \"sampstat\")$cov\n\n# Obtener matriz de correlaciones reproducidas por el modelo\nr_model    &lt;-  lavInspect(CFA.results.train, \"implied\")$cov\n\n# Calcular matriz de residuos estandarizados\nresidual_matrix &lt;- r_observed - r_model\n\nprint(residual_matrix)\n\n       TSC1   TSC2   TSC3   TSC5    TE1    TE2    TE3    TE5    EE1    EE2\nTSC1  0.000                                                               \nTSC2 -0.012  0.000                                                        \nTSC3  0.007  0.012  0.000                                                 \nTSC5  0.007 -0.002 -0.010  0.000                                          \nTE1  -0.019  0.000 -0.009  0.010  0.000                                   \nTE2   0.025 -0.014 -0.031  0.021  0.011  0.000                            \nTE3   0.013 -0.010 -0.048 -0.005  0.003  0.008  0.000                     \nTE5   0.025  0.028  0.032  0.022 -0.012 -0.026 -0.005  0.000              \nEE1  -0.013  0.010  0.004 -0.016  0.042  0.044  0.001  0.072  0.000       \nEE2   0.004 -0.009  0.025 -0.003 -0.029 -0.050 -0.027  0.043 -0.002  0.000\nEE3  -0.013  0.015 -0.039  0.013 -0.021 -0.042 -0.006  0.081 -0.012 -0.001\nEE4  -0.002  0.002  0.000  0.013 -0.042  0.021  0.006  0.039 -0.017  0.017\nDE1  -0.011 -0.019  0.015  0.002  0.010 -0.026 -0.011  0.036 -0.010 -0.048\nDE2  -0.014  0.018  0.030 -0.011 -0.008 -0.025 -0.032  0.059  0.058  0.031\nDE3   0.000 -0.008  0.041 -0.021  0.023  0.006 -0.012  0.019  0.048 -0.015\nRPA1 -0.008  0.015 -0.034  0.011 -0.013 -0.022 -0.001  0.012  0.011 -0.018\nRPA2 -0.006 -0.008 -0.044  0.007 -0.021  0.004 -0.009  0.008  0.015 -0.016\nRPA3  0.041  0.016 -0.012  0.003 -0.006  0.010  0.017  0.034  0.035  0.008\nRPA4  0.020  0.000  0.003  0.031  0.001  0.027  0.031  0.039  0.042  0.035\n        EE3    EE4    DE1    DE2    DE3   RPA1   RPA2   RPA3   RPA4\nTSC1                                                               \nTSC2                                                               \nTSC3                                                               \nTSC5                                                               \nTE1                                                                \nTE2                                                                \nTE3                                                                \nTE5                                                                \nEE1                                                                \nEE2                                                                \nEE3   0.000                                                        \nEE4   0.010  0.000                                                 \nDE1  -0.042 -0.040  0.000                                          \nDE2   0.012  0.052  0.004  0.000                                   \nDE3  -0.022  0.012  0.002 -0.006  0.000                            \nRPA1  0.019 -0.041 -0.008 -0.006  0.002  0.000                     \nRPA2 -0.002 -0.053 -0.010 -0.025 -0.024  0.024  0.000              \nRPA3  0.022 -0.009  0.002  0.016  0.021 -0.009 -0.017  0.000       \nRPA4  0.031  0.053  0.006  0.056  0.074 -0.046 -0.011  0.052  0.000\n\nmax(residual_matrix)\n\n[1] 0.08051991\n\n\n\n\n\n\n\nsi el modelo ajusta bien tambien en el conjunto de datos de entrenamiento, puedes estar seguro que el modelo realmente captura bien la estructura factorial subyacente\n\n\nCFA.results.test &lt;- cfa(model        = CFA_model,\n                       data          = db_test,\n                       estimator     = \"MLR\",\n                       std.lv        = TRUE,\n                       meanstructure = TRUE)"
  }
]