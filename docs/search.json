[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Jesus Quispe\n\nI’m a Economist (PUCP). I worked in research and the insurance sector. I blog about statistics, programming, and data analytics applied in quantitative finance, actuarial science or economics.\nI have extensive experience with causal inference, regression analysis, spatial analysis, econometrics, actuarial analytics.\nI excel at applied statistics, especially developing new methods to analyze complex data sets.\nI love to collaborate with scientists and product teams on involved projects.\nYou can find my resume here. I’m currently based in Lima,Peru"
  },
  {
    "objectID": "Articles/2019-05-10_Beauty_ggplots/2019-05-10_Beauty_ggplots.html",
    "href": "Articles/2019-05-10_Beauty_ggplots/2019-05-10_Beauty_ggplots.html",
    "title": "Plotting with Ggplot2:Barplot",
    "section": "",
    "text": "Plot beauty barplots\nWe had data of admissions to UCB\n\n\n\n\n\nAdmit\nGender\nDept\nFreq\n\n\n\n\nAdmitted\nMale\nA\n512\n\n\nRejected\nMale\nA\n313\n\n\nAdmitted\nFemale\nA\n89\n\n\nRejected\nFemale\nA\n19\n\n\nAdmitted\nMale\nB\n353\n\n\nRejected\nMale\nB\n207\n\n\nAdmitted\nFemale\nB\n17\n\n\nRejected\nFemale\nB\n8\n\n\nAdmitted\nMale\nC\n120\n\n\nRejected\nMale\nC\n205\n\n\nAdmitted\nFemale\nC\n202\n\n\nRejected\nFemale\nC\n391\n\n\nAdmitted\nMale\nD\n138\n\n\nRejected\nMale\nD\n279\n\n\nAdmitted\nFemale\nD\n131\n\n\nRejected\nFemale\nD\n244\n\n\nAdmitted\nMale\nE\n53\n\n\nRejected\nMale\nE\n138\n\n\nAdmitted\nFemale\nE\n94\n\n\nRejected\nFemale\nE\n299\n\n\nAdmitted\nMale\nF\n22\n\n\nRejected\nMale\nF\n351\n\n\nAdmitted\nFemale\nF\n24\n\n\nRejected\nFemale\nF\n317\n\n\n\n\n\nA first approach\n\n\n\n\n\nPlot upgraded"
  },
  {
    "objectID": "Articles/2022-01-10_Time_series/Time_series.html",
    "href": "Articles/2022-01-10_Time_series/Time_series.html",
    "title": "Plotting with Ggplot2:Time series",
    "section": "",
    "text": "Plotting Time series with ggplot\n#Packages\n\nLoad Data\n\n\nplot"
  },
  {
    "objectID": "Articles.html",
    "href": "Articles.html",
    "title": "Articles",
    "section": "",
    "text": "Plotting with Ggplot2:Time series\n\n\nTime series\n\n\n\n\n\n\n\n\n\nJan 10, 2022\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nAprendizaje automatico usando Tidymodels\n\n\nMachine learning & Quantitative finance\n\n\nDescription of how to use tidymodels to traing Machine learning models.\n\n\n\n\n\n\nJun 10, 2021\n\n\n14 min\n\n\n\n\n\n\n  \n\n\n\n\nPlotting with Ggplot2:Barplot\n\n\nBarplot\n\n\n\n\n\n\n\n\n\nMay 10, 2019\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jesus Quispe Q.",
    "section": "",
    "text": "Page of Jesus Miguel Quispe Quispe"
  },
  {
    "objectID": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html",
    "href": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html",
    "title": "Aprendizaje automatico usando Tidymodels",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(gt)\n\n\n\n\nUno de los modelos clasicos para llevar acabo el pricing de opciones es el modelo black-sholes el cual tipicamente plantea la siguiente entidad :\n\\[ C(S_0,t) = S_0 N(d_1) - Ke^{-r(T-t)}N(d_2)\\]\n\\[ d_1 =     \\frac{(ln\\frac{S_{0}}{K} + (r +\\frac{σ^2}{2} )(T-t))}{σ    \\sqrt{T-t}} \\]\n\\[d_2 = d_1 - σ \\sqrt{T-t}\\]\nDonde:\n\n\\(S_0\\): Precio del subyacente (Stock Price)\n\\(C(S0,t)\\): Price of the Call Option\n\\(K\\): Exercise Price\n\\((T−t)\\): Tiempo de maduracion, donde T es la fecha de ejercicio(Time to Maturity, where T is Exercise Date)\n\\(σ\\): Volatilidad subyacente (Underlying Volatility (a standard deviation of log returns))\n\\(r\\): Tasa de interes libre de riesgo (Risk-free Interest Rate (i.e., T-bill Rate))\n\nEsta ecuacion puede ser formulada en R del siguiente modo :\n\nblack_scholes_price &lt;- function(S, K = 70, r = 0, T = 1, sigma = 0.2) {\n  \n  d1     &lt;- (log(S / K) + (r + sigma^2 / 2) * T) / (sigma * sqrt(T))\n  d2     &lt;-  d1 - sigma * sqrt(T)\n  price  &lt;-  S * pnorm(d1) - K * exp(-r * T) * pnorm(d2)\n  \n  return(price)\n}\n\nEmpleando la anterior expresion, podemos simular precios de opciones;asi como una base de datos de sus determinantes. Dado lo anterior, tendremos un escenario perfecto para llevar acabo metodologias asociadas al aprendizaje automatico o ML ya que podriamos calcular el predecir el precio de una opcion financiera . Asi partimos simulando la base de datos necesaria para el problema de regresion que se nos presenta\n\nset.seed(420)\n\noption_prices &lt;- expand_grid(\n  S = 40:50,\n  K = 20:40,\n  r = seq(from = 0, to = 0.03, by = 0.01),\n  T = seq(from = 3 / 12, to = 1.5, by = 1 / 12),\n  sigma = seq(from = 0.1, to = 0.3, by = 0.1)\n) |&gt;\n  mutate(\n    black_scholes = black_scholes_price(S, K, r, T, sigma),\n    observed_price = map_dbl(\n      black_scholes,\n      function(x) x + rnorm(1, sd = 0.15)\n    )\n  )\n\n  option_prices |&gt; \n  head(10)  |&gt;\n  gt() \n\n\n\n\n\n  \n    \n    \n      S\n      K\n      r\n      T\n      sigma\n      black_scholes\n      observed_price\n    \n  \n  \n    40\n20\n0\n0.2500000\n0.1\n20.00000\n20.04016\n    40\n20\n0\n0.2500000\n0.2\n20.00000\n19.85949\n    40\n20\n0\n0.2500000\n0.3\n20.00000\n20.08943\n    40\n20\n0\n0.3333333\n0.1\n20.00000\n19.95319\n    40\n20\n0\n0.3333333\n0.2\n20.00000\n20.05970\n    40\n20\n0\n0.3333333\n0.3\n20.00003\n19.92282\n    40\n20\n0\n0.4166667\n0.1\n20.00000\n19.91722\n    40\n20\n0\n0.4166667\n0.2\n20.00000\n19.93092\n    40\n20\n0\n0.4166667\n0.3\n20.00023\n19.55791\n    40\n20\n0\n0.5000000\n0.1\n20.00000\n19.88493\n  \n  \n  \n\n\n\n\nInmediatamente procedemos a establecer el conjunto de datos de entrenamiento y testeo. Asi como establecemos la metodologia de V-fold validacion cruzada sobre el conjunto de datos de entrenamiento\n\n# 40 -60% \nsplit         &lt;- initial_split(option_prices, prop = 0.40)\noption_train  &lt;- training(split)\noption_test   &lt;- testing(split)\n\n\n\n\nset.seed(123)\noption_folds &lt;- vfold_cv(option_train, v = 20)\n\n\n\n\nLuego procedemos a plantear los potenciales predictores del precio de la opcion; asi como el procesamiento basico de los mismo o Feature engineering .Para ello empleamos una ‘recipe’\n\nrec.option &lt;- recipe(observed_price ~ .,\n              data = option_prices\n              ) |&gt;\n  step_rm(black_scholes) |&gt;\n  step_normalize(all_predictors())\n\n\n\n\nDefinimos el set de modelos que emplearemos y evaluaremos su performance\n\nmars_model &lt;- mars(  num_terms = tune(),\n                       prod_degree = tune(),\n                       prune_method = tune())  |&gt;\n                set_engine(\"earth\")            |&gt;\n                set_mode(\"regression\")\n\n\nlibrary(bonsai)\nrf_model &lt;-   rand_forest(mtry  = tune(), \n                           min_n  = tune(), trees = 100)            |&gt;\n              set_engine(\"ranger\")    |&gt;\n              set_mode(\"regression\")\n\n\n\n\nlgbm_model &lt;-   boost_tree(learn_rate = tune(), stop_iter = tune(),\n                            trees = 100) %&gt;%\n                set_engine(\"lightgbm\", num_leaves = tune()) %&gt;%\n                set_mode(\"regression\")\n\n\n\n\n\nmars_wflow   &lt;- workflow(rec.option, mars_model)\nrf_wflow     &lt;- workflow(rec.option, rf_model)\nlgbm_wflow   &lt;- workflow(rec.option, lgbm_model)\n\nLuego procedemos a ajustar el modelo planteado sobre los ‘folds’ creados, mediante la metodologia de validacion cruzada de manera que se busca optimizar los hiperparametros de los modelos planteados\n\nset.seed(123)\nmars_time_grid &lt;- system.time(\n                  mars_res_grid &lt;- tune_grid(mars_wflow, option_folds, grid = 5)\n)\n\n\nset.seed(123)\nrf_time_grid&lt;- system.time(\n                  rf_res_grid &lt;- tune_grid(rf_wflow, option_folds, grid = 5)\n)\n\n\nset.seed(123)\nlgbm_time_grid &lt;- system.time(\n                  lgbm_res_grid &lt;- tune_grid(lgbm_wflow, option_folds, grid = 5)\n)\n\n\n\n\nProcedemos a seleccionar el mejor modelo empleando para ello la metrica de error de la raiz cuadrada media estandarizada o rsme( por sus siglas en ingles )\n\nrf_rmse       &lt;- rf_res_grid %&gt;%\n                 select_best(\"rmse\", maximize = FALSE)\n\nlgbm_rmse     &lt;- lgbm_res_grid %&gt;%\n                 select_best(\"rmse\", maximize = FALSE)\n\nmars_rmse     &lt;- mars_res_grid %&gt;%\n                 select_best(\"rmse\", maximize = FALSE)\n\n\n\n\nfinal_mars &lt;- finalize_workflow(\n              mars_wflow,\n              mars_rmse\n)\n\nfinal_rf &lt;- finalize_workflow(\n              rf_wflow,\n              rf_rmse\n)\n\nfinal_lgbm &lt;- finalize_workflow(\n              lgbm_wflow,\n              lgbm_rmse\n)\n\n\n\nFinalmente reorganizamos los resultados de las metricas de error para facilitar la seleccion del mejor modelo.\n\nm_mars_db  &lt;- last_fit(\n              final_mars,\n              split\n              ) %&gt;%\n              collect_metrics() %&gt;% \n              mutate(.model =\"mars\")\n\n\nm_rf_db  &lt;- last_fit(\n            final_rf,\n            split\n            ) %&gt;%\n            collect_metrics() %&gt;% \n              mutate(.model =\"Random.Forest\")\n\nm_lgbm_db  &lt;- last_fit(\n              final_lgbm,\n              split\n              ) %&gt;%\n              collect_metrics()%&gt;% \n              mutate(.model =\"Light.gbm\")\n\n\nmtrcs_db   &lt;-rbind(m_mars_db,\n                   m_rf_db,\n                   m_lgbm_db)\n\n\nmtrcs_db                                                    |&gt; \nselect(.metric,.estimate,.model)                            |&gt; \npivot_wider(names_from = .metric,values_from =  .estimate)  |&gt; \ngt()                                                        |&gt;\ntab_header(\n    title = md(\"**Metricas de error**\"),\n    subtitle = \"Pricing opciones\"\n  )                                                         |&gt;\n  tab_options(\n    table.background.color = \"white\",\n    column_labels.background.color = \"white\",\n    table.font.size = px(15),\n    column_labels.font.size = px(17),\n    row.striping.background_color = \"gray\",\n    heading.align = \"center\",\n    heading.title.font.size = px(20)\n  ) %&gt;% \n  opt_row_striping() \n\n\n\n\nPreprocesamiento del error de prediccion de cada modelo\n\nout_of_sample_data &lt;- testing(split)                  |&gt;\n                      slice_sample(n = 10000)\n\ndim(out_of_sample_data)\n\npredictive_performance &lt;- final_mars                  |&gt;         # best workflow\n                          fit(data = training(split)) |&gt;         # fitted with full training dataset\n                          predict(out_of_sample_data) |&gt;           # predict \n                          rename(\"mars\" = .pred)      |&gt; \n                          bind_cols(\n                              final_rf                      |&gt;         \n                              fit(data = training(split))   |&gt;         \n                              predict(out_of_sample_data)   |&gt;            \n                              rename(\"Random.Forest\" = .pred) \n                            )                               |&gt; \n                          bind_cols(\n                              final_rf                      |&gt;         \n                              fit(data = training(split))   |&gt;         \n                              predict(out_of_sample_data)   |&gt;            \n                              rename(\"LGBM\" = .pred) \n                            )                               |&gt;\n  bind_cols(out_of_sample_data)                             |&gt;\n  pivot_longer(\"mars\":\"LGBM\", names_to = \"Model\")           |&gt;\n    mutate(\n    moneyness = (S - K),\n    pricing_error = abs(value - black_scholes)\n  )\n\nVisualizacion final del error de prediccion de los modelos anteriormente evaluados\n\npredictive_performance |&gt;\n  ggplot(aes(\n    x = moneyness, \n    y = pricing_error, \n    color = Model,\n    linetype = Model\n    )) +\n  geom_jitter(alpha = 0.05) +\n  scale_color_viridis_d()+\n  geom_smooth(se = FALSE, method = \"gam\", formula = y ~ s(x, bs = \"cs\")) +\n  labs(\n    x = \"Moneyness (S - K)\", color = NULL,\n    y = \"Error de prediccion\",\n    title = \"Error de prediccion de modelos opciones de compra\",\n    linetype = NULL\n  )+theme_bw()+\n  theme(legend.position = \"bottom\",\n        plot.title = element_text(hjust = 0.5))"
  },
  {
    "objectID": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#modelo-black--sholes",
    "href": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#modelo-black--sholes",
    "title": "Aprendizaje automatico usando Tidymodels",
    "section": "",
    "text": "Uno de los modelos clasicos para llevar acabo el pricing de opciones es el modelo black-sholes el cual tipicamente plantea la siguiente entidad :\n\\[ C(S_0,t) = S_0 N(d_1) - Ke^{-r(T-t)}N(d_2)\\]\n\\[ d_1 =     \\frac{(ln\\frac{S_{0}}{K} + (r +\\frac{σ^2}{2} )(T-t))}{σ    \\sqrt{T-t}} \\]\n\\[d_2 = d_1 - σ \\sqrt{T-t}\\]\nDonde:\n\n\\(S_0\\): Precio del subyacente (Stock Price)\n\\(C(S0,t)\\): Price of the Call Option\n\\(K\\): Exercise Price\n\\((T−t)\\): Tiempo de maduracion, donde T es la fecha de ejercicio(Time to Maturity, where T is Exercise Date)\n\\(σ\\): Volatilidad subyacente (Underlying Volatility (a standard deviation of log returns))\n\\(r\\): Tasa de interes libre de riesgo (Risk-free Interest Rate (i.e., T-bill Rate))\n\nEsta ecuacion puede ser formulada en R del siguiente modo :\n\nblack_scholes_price &lt;- function(S, K = 70, r = 0, T = 1, sigma = 0.2) {\n  \n  d1     &lt;- (log(S / K) + (r + sigma^2 / 2) * T) / (sigma * sqrt(T))\n  d2     &lt;-  d1 - sigma * sqrt(T)\n  price  &lt;-  S * pnorm(d1) - K * exp(-r * T) * pnorm(d2)\n  \n  return(price)\n}\n\nEmpleando la anterior expresion, podemos simular precios de opciones;asi como una base de datos de sus determinantes. Dado lo anterior, tendremos un escenario perfecto para llevar acabo metodologias asociadas al aprendizaje automatico o ML ya que podriamos calcular el predecir el precio de una opcion financiera . Asi partimos simulando la base de datos necesaria para el problema de regresion que se nos presenta\n\nset.seed(420)\n\noption_prices &lt;- expand_grid(\n  S = 40:50,\n  K = 20:40,\n  r = seq(from = 0, to = 0.03, by = 0.01),\n  T = seq(from = 3 / 12, to = 1.5, by = 1 / 12),\n  sigma = seq(from = 0.1, to = 0.3, by = 0.1)\n) |&gt;\n  mutate(\n    black_scholes = black_scholes_price(S, K, r, T, sigma),\n    observed_price = map_dbl(\n      black_scholes,\n      function(x) x + rnorm(1, sd = 0.15)\n    )\n  )\n\n  option_prices |&gt; \n  head(10)  |&gt;\n  gt() \n\n\n\n\n\n  \n    \n    \n      S\n      K\n      r\n      T\n      sigma\n      black_scholes\n      observed_price\n    \n  \n  \n    40\n20\n0\n0.2500000\n0.1\n20.00000\n20.04016\n    40\n20\n0\n0.2500000\n0.2\n20.00000\n19.85949\n    40\n20\n0\n0.2500000\n0.3\n20.00000\n20.08943\n    40\n20\n0\n0.3333333\n0.1\n20.00000\n19.95319\n    40\n20\n0\n0.3333333\n0.2\n20.00000\n20.05970\n    40\n20\n0\n0.3333333\n0.3\n20.00003\n19.92282\n    40\n20\n0\n0.4166667\n0.1\n20.00000\n19.91722\n    40\n20\n0\n0.4166667\n0.2\n20.00000\n19.93092\n    40\n20\n0\n0.4166667\n0.3\n20.00023\n19.55791\n    40\n20\n0\n0.5000000\n0.1\n20.00000\n19.88493\n  \n  \n  \n\n\n\n\nInmediatamente procedemos a establecer el conjunto de datos de entrenamiento y testeo. Asi como establecemos la metodologia de V-fold validacion cruzada sobre el conjunto de datos de entrenamiento\n\n# 40 -60% \nsplit         &lt;- initial_split(option_prices, prop = 0.40)\noption_train  &lt;- training(split)\noption_test   &lt;- testing(split)\n\n\n\n\nset.seed(123)\noption_folds &lt;- vfold_cv(option_train, v = 20)"
  },
  {
    "objectID": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#definicion-del-predictor",
    "href": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#definicion-del-predictor",
    "title": "Aprendizaje automatico usando Tidymodels",
    "section": "",
    "text": "Luego procedemos a plantear los potenciales predictores del precio de la opcion; asi como el procesamiento basico de los mismo o Feature engineering .Para ello empleamos una ‘recipe’\n\nrec.option &lt;- recipe(observed_price ~ .,\n              data = option_prices\n              ) |&gt;\n  step_rm(black_scholes) |&gt;\n  step_normalize(all_predictors())"
  },
  {
    "objectID": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#definicion-del-modelo-o-engine",
    "href": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#definicion-del-modelo-o-engine",
    "title": "Aprendizaje automatico usando Tidymodels",
    "section": "",
    "text": "Definimos el set de modelos que emplearemos y evaluaremos su performance\n\nmars_model &lt;- mars(  num_terms = tune(),\n                       prod_degree = tune(),\n                       prune_method = tune())  |&gt;\n                set_engine(\"earth\")            |&gt;\n                set_mode(\"regression\")\n\n\nlibrary(bonsai)\nrf_model &lt;-   rand_forest(mtry  = tune(), \n                           min_n  = tune(), trees = 100)            |&gt;\n              set_engine(\"ranger\")    |&gt;\n              set_mode(\"regression\")\n\n\n\n\nlgbm_model &lt;-   boost_tree(learn_rate = tune(), stop_iter = tune(),\n                            trees = 100) %&gt;%\n                set_engine(\"lightgbm\", num_leaves = tune()) %&gt;%\n                set_mode(\"regression\")"
  },
  {
    "objectID": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#definicion-de-los-flujos-de-trabajo",
    "href": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#definicion-de-los-flujos-de-trabajo",
    "title": "Aprendizaje automatico usando Tidymodels",
    "section": "",
    "text": "mars_wflow   &lt;- workflow(rec.option, mars_model)\nrf_wflow     &lt;- workflow(rec.option, rf_model)\nlgbm_wflow   &lt;- workflow(rec.option, lgbm_model)\n\nLuego procedemos a ajustar el modelo planteado sobre los ‘folds’ creados, mediante la metodologia de validacion cruzada de manera que se busca optimizar los hiperparametros de los modelos planteados\n\nset.seed(123)\nmars_time_grid &lt;- system.time(\n                  mars_res_grid &lt;- tune_grid(mars_wflow, option_folds, grid = 5)\n)\n\n\nset.seed(123)\nrf_time_grid&lt;- system.time(\n                  rf_res_grid &lt;- tune_grid(rf_wflow, option_folds, grid = 5)\n)\n\n\nset.seed(123)\nlgbm_time_grid &lt;- system.time(\n                  lgbm_res_grid &lt;- tune_grid(lgbm_wflow, option_folds, grid = 5)\n)"
  },
  {
    "objectID": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#mejor-modelo",
    "href": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#mejor-modelo",
    "title": "Aprendizaje automatico usando Tidymodels",
    "section": "",
    "text": "Procedemos a seleccionar el mejor modelo empleando para ello la metrica de error de la raiz cuadrada media estandarizada o rsme( por sus siglas en ingles )\n\nrf_rmse       &lt;- rf_res_grid %&gt;%\n                 select_best(\"rmse\", maximize = FALSE)\n\nlgbm_rmse     &lt;- lgbm_res_grid %&gt;%\n                 select_best(\"rmse\", maximize = FALSE)\n\nmars_rmse     &lt;- mars_res_grid %&gt;%\n                 select_best(\"rmse\", maximize = FALSE)\n\n\n\n\nfinal_mars &lt;- finalize_workflow(\n              mars_wflow,\n              mars_rmse\n)\n\nfinal_rf &lt;- finalize_workflow(\n              rf_wflow,\n              rf_rmse\n)\n\nfinal_lgbm &lt;- finalize_workflow(\n              lgbm_wflow,\n              lgbm_rmse\n)\n\n\n\nFinalmente reorganizamos los resultados de las metricas de error para facilitar la seleccion del mejor modelo.\n\nm_mars_db  &lt;- last_fit(\n              final_mars,\n              split\n              ) %&gt;%\n              collect_metrics() %&gt;% \n              mutate(.model =\"mars\")\n\n\nm_rf_db  &lt;- last_fit(\n            final_rf,\n            split\n            ) %&gt;%\n            collect_metrics() %&gt;% \n              mutate(.model =\"Random.Forest\")\n\nm_lgbm_db  &lt;- last_fit(\n              final_lgbm,\n              split\n              ) %&gt;%\n              collect_metrics()%&gt;% \n              mutate(.model =\"Light.gbm\")\n\n\nmtrcs_db   &lt;-rbind(m_mars_db,\n                   m_rf_db,\n                   m_lgbm_db)\n\n\n\n\n\n\n\n  \n    \n      Metricas de error\n    \n    \n      Pricing opciones\n    \n    \n      .model\n      rmse\n      rsq\n    \n  \n  \n    mars\n1.2432680\n0.9627034\n    Random.Forest\n0.2153714\n0.9988867\n    Light.gbm\n5.0747239\n0.9848114"
  },
  {
    "objectID": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#carga-de-paquetes",
    "href": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#carga-de-paquetes",
    "title": "Aprendizaje automatico usando Tidymodels",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(gt)"
  },
  {
    "objectID": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#evaluacion-del-mejor-modelo",
    "href": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#evaluacion-del-mejor-modelo",
    "title": "Aprendizaje automatico usando Tidymodels",
    "section": "",
    "text": "Procedemos a seleccionar el mejor modelo empleando para ello la metrica de error de la raiz cuadrada media estandarizada o rsme( por sus siglas en ingles )\n\nrf_rmse       &lt;- rf_res_grid %&gt;%\n                 select_best(\"rmse\", maximize = FALSE)\n\nlgbm_rmse     &lt;- lgbm_res_grid %&gt;%\n                 select_best(\"rmse\", maximize = FALSE)\n\nmars_rmse     &lt;- mars_res_grid %&gt;%\n                 select_best(\"rmse\", maximize = FALSE)\n\n\n\n\nfinal_mars &lt;- finalize_workflow(\n              mars_wflow,\n              mars_rmse\n)\n\nfinal_rf &lt;- finalize_workflow(\n              rf_wflow,\n              rf_rmse\n)\n\nfinal_lgbm &lt;- finalize_workflow(\n              lgbm_wflow,\n              lgbm_rmse\n)\n\n\n\nFinalmente reorganizamos los resultados de las metricas de error para facilitar la seleccion del mejor modelo.\n\nm_mars_db  &lt;- last_fit(\n              final_mars,\n              split\n              ) %&gt;%\n              collect_metrics() %&gt;% \n              mutate(.model =\"mars\")\n\n\nm_rf_db  &lt;- last_fit(\n            final_rf,\n            split\n            ) %&gt;%\n            collect_metrics() %&gt;% \n              mutate(.model =\"Random.Forest\")\n\nm_lgbm_db  &lt;- last_fit(\n              final_lgbm,\n              split\n              ) %&gt;%\n              collect_metrics()%&gt;% \n              mutate(.model =\"Light.gbm\")\n\n\nmtrcs_db   &lt;-rbind(m_mars_db,\n                   m_rf_db,\n                   m_lgbm_db)\n\n\nmtrcs_db                                                    |&gt; \nselect(.metric,.estimate,.model)                            |&gt; \npivot_wider(names_from = .metric,values_from =  .estimate)  |&gt; \ngt()                                                        |&gt;\ntab_header(\n    title = md(\"**Metricas de error**\"),\n    subtitle = \"Pricing opciones\"\n  )                                                         |&gt;\n  tab_options(\n    table.background.color = \"white\",\n    column_labels.background.color = \"white\",\n    table.font.size = px(15),\n    column_labels.font.size = px(17),\n    row.striping.background_color = \"gray\",\n    heading.align = \"center\",\n    heading.title.font.size = px(20)\n  ) %&gt;% \n  opt_row_striping() \n\n\n\n\nPreprocesamiento del error de prediccion de cada modelo\n\nout_of_sample_data &lt;- testing(split)                  |&gt;\n                      slice_sample(n = 10000)\n\ndim(out_of_sample_data)\n\npredictive_performance &lt;- final_mars                  |&gt;         # best workflow\n                          fit(data = training(split)) |&gt;         # fitted with full training dataset\n                          predict(out_of_sample_data) |&gt;           # predict \n                          rename(\"mars\" = .pred)      |&gt; \n                          bind_cols(\n                              final_rf                      |&gt;         \n                              fit(data = training(split))   |&gt;         \n                              predict(out_of_sample_data)   |&gt;            \n                              rename(\"Random.Forest\" = .pred) \n                            )                               |&gt; \n                          bind_cols(\n                              final_rf                      |&gt;         \n                              fit(data = training(split))   |&gt;         \n                              predict(out_of_sample_data)   |&gt;            \n                              rename(\"LGBM\" = .pred) \n                            )                               |&gt;\n  bind_cols(out_of_sample_data)                             |&gt;\n  pivot_longer(\"mars\":\"LGBM\", names_to = \"Model\")           |&gt;\n    mutate(\n    moneyness = (S - K),\n    pricing_error = abs(value - black_scholes)\n  )\n\nVisualizacion final del error de prediccion de los modelos anteriormente evaluados\n\npredictive_performance |&gt;\n  ggplot(aes(\n    x = moneyness, \n    y = pricing_error, \n    color = Model,\n    linetype = Model\n    )) +\n  geom_jitter(alpha = 0.05) +\n  scale_color_viridis_d()+\n  geom_smooth(se = FALSE, method = \"gam\", formula = y ~ s(x, bs = \"cs\")) +\n  labs(\n    x = \"Moneyness (S - K)\", color = NULL,\n    y = \"Error de prediccion\",\n    title = \"Error de prediccion de modelos opciones de compra\",\n    linetype = NULL\n  )+theme_bw()+\n  theme(legend.position = \"bottom\",\n        plot.title = element_text(hjust = 0.5))"
  }
]