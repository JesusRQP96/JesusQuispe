[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Jesus Quispe\n\nI’m a Economist (PUCP). I worked in research and the insurance sector. I blog about statistics, programming, and data analytics applied in quantitative finance, actuarial science or economics.\nI have extensive experience with causal inference, regression analysis, spatial analysis, econometrics, actuarial analytics.\nI excel at applied statistics, especially developing new methods to analyze complex data sets.\nI love to collaborate with scientists and product teams on involved projects.\nYou can find my resume here. I’m currently based in Lima,Peru"
  },
  {
    "objectID": "Articles.html",
    "href": "Articles.html",
    "title": "Articles",
    "section": "",
    "text": "Bayesian variable selection\n\n\nDrivers of wage\n\n\nDescription of how to apply the bayesian variable selection approach\n\n\n\n\n\n\nMar 16, 2024\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nPlotting with Ggplot2:Time series\n\n\nTime series\n\n\nDescription of how make beauty time series plots\n\n\n\n\n\n\nJan 10, 2022\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nAprendizaje automatico usando Tidymodels\n\n\nMachine learning & Quantitative finance\n\n\nDescription of how to use tidymodels to training Machine learning models.\n\n\n\n\n\n\nJun 10, 2021\n\n\n14 min\n\n\n\n\n\n\n  \n\n\n\n\nPlotting with Ggplot2:Barplot\n\n\nBarplot\n\n\nDescription of how make beauty barplots\n\n\n\n\n\n\nMay 10, 2019\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Articles/2019-05-10_Beauty_ggplots/2019-05-10_Beauty_ggplots.html",
    "href": "Articles/2019-05-10_Beauty_ggplots/2019-05-10_Beauty_ggplots.html",
    "title": "Plotting with Ggplot2:Barplot",
    "section": "",
    "text": "Plot beauty barplots\nWe had data of admissions to UCB\n\nlibrary(tidyverse)\nlibrary(knitr)\n\n\ndb.df&lt;-data.frame(UCBAdmissions)\n\nkable(db.df)\n\n\n\n\nAdmit\nGender\nDept\nFreq\n\n\n\n\nAdmitted\nMale\nA\n512\n\n\nRejected\nMale\nA\n313\n\n\nAdmitted\nFemale\nA\n89\n\n\nRejected\nFemale\nA\n19\n\n\nAdmitted\nMale\nB\n353\n\n\nRejected\nMale\nB\n207\n\n\nAdmitted\nFemale\nB\n17\n\n\nRejected\nFemale\nB\n8\n\n\nAdmitted\nMale\nC\n120\n\n\nRejected\nMale\nC\n205\n\n\nAdmitted\nFemale\nC\n202\n\n\nRejected\nFemale\nC\n391\n\n\nAdmitted\nMale\nD\n138\n\n\nRejected\nMale\nD\n279\n\n\nAdmitted\nFemale\nD\n131\n\n\nRejected\nFemale\nD\n244\n\n\nAdmitted\nMale\nE\n53\n\n\nRejected\nMale\nE\n138\n\n\nAdmitted\nFemale\nE\n94\n\n\nRejected\nFemale\nE\n299\n\n\nAdmitted\nMale\nF\n22\n\n\nRejected\nMale\nF\n351\n\n\nAdmitted\nFemale\nF\n24\n\n\nRejected\nFemale\nF\n317\n\n\n\n\n\nA first approach\n\ndb.df                                                   %&gt;% \nggplot()                                                 +\ngeom_col(aes(Admit,Freq,fill =Gender),\n         position = position_stack(reverse = FALSE),\n         lwd = 3)     +\nfacet_wrap(vars(Dept)) +\ntheme_bw()+\nxlab(\"Admition\") +\nylab(\"Frequency\")\n\n\n\n\nPlot upgraded\n\ndb.df                                                   %&gt;% \nggplot()                                                 +\ngeom_col(aes(Admit,Freq,fill =Gender,color=Gender),alpha=0.5,\n         position = position_stack(reverse = FALSE),\n         lwd = 1.2)     +\nfacet_wrap(vars(Dept)) +\nxlab(\"Admition\") +\nylab(\"Frequency\")+\nscale_fill_viridis_d() +\nscale_color_viridis_d()+\ntheme_dark()"
  },
  {
    "objectID": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html",
    "href": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html",
    "title": "Aprendizaje automatico usando Tidymodels",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(gt)\n\n\n\n\nUno de los modelos clasicos para llevar acabo el pricing de opciones es el modelo black-sholes el cual tipicamente plantea la siguiente entidad :\n\\[ C(S_0,t) = S_0 N(d_1) - Ke^{-r(T-t)}N(d_2)\\]\n\\[ d_1 =     \\frac{(ln\\frac{S_{0}}{K} + (r +\\frac{σ^2}{2} )(T-t))}{σ    \\sqrt{T-t}} \\]\n\\[d_2 = d_1 - σ \\sqrt{T-t}\\]\nDonde:\n\n\\(S_0\\): Precio del subyacente (Stock Price)\n\\(C(S0,t)\\): Price of the Call Option\n\\(K\\): Exercise Price\n\\((T−t)\\): Tiempo de maduracion, donde T es la fecha de ejercicio(Time to Maturity, where T is Exercise Date)\n\\(σ\\): Volatilidad subyacente (Underlying Volatility (a standard deviation of log returns))\n\\(r\\): Tasa de interes libre de riesgo (Risk-free Interest Rate (i.e., T-bill Rate))\n\nEsta ecuacion puede ser formulada en R del siguiente modo :\n\nblack_scholes_price &lt;- function(S, K = 70, r = 0, T = 1, sigma = 0.2) {\n  \n  d1     &lt;- (log(S / K) + (r + sigma^2 / 2) * T) / (sigma * sqrt(T))\n  d2     &lt;-  d1 - sigma * sqrt(T)\n  price  &lt;-  S * pnorm(d1) - K * exp(-r * T) * pnorm(d2)\n  \n  return(price)\n}\n\nEmpleando la anterior expresion, podemos simular precios de opciones;asi como una base de datos de sus determinantes. Dado lo anterior, tendremos un escenario perfecto para llevar acabo metodologias asociadas al aprendizaje automatico o ML ya que podriamos calcular el predecir el precio de una opcion financiera . Asi partimos simulando la base de datos necesaria para el problema de regresion que se nos presenta\n\nset.seed(420)\n\noption_prices &lt;- expand_grid(\n  S = 40:50,\n  K = 20:40,\n  r = seq(from = 0, to = 0.03, by = 0.01),\n  T = seq(from = 3 / 12, to = 1.5, by = 1 / 12),\n  sigma = seq(from = 0.1, to = 0.3, by = 0.1)\n) |&gt;\n  mutate(\n    black_scholes = black_scholes_price(S, K, r, T, sigma),\n    observed_price = map_dbl(\n      black_scholes,\n      function(x) x + rnorm(1, sd = 0.15)\n    )\n  )\n\n  option_prices |&gt; \n  head(10)  |&gt;\n  gt() \n\n\n\n\n\n  \n    \n    \n      S\n      K\n      r\n      T\n      sigma\n      black_scholes\n      observed_price\n    \n  \n  \n    40\n20\n0\n0.2500000\n0.1\n20.00000\n20.04016\n    40\n20\n0\n0.2500000\n0.2\n20.00000\n19.85949\n    40\n20\n0\n0.2500000\n0.3\n20.00000\n20.08943\n    40\n20\n0\n0.3333333\n0.1\n20.00000\n19.95319\n    40\n20\n0\n0.3333333\n0.2\n20.00000\n20.05970\n    40\n20\n0\n0.3333333\n0.3\n20.00003\n19.92282\n    40\n20\n0\n0.4166667\n0.1\n20.00000\n19.91722\n    40\n20\n0\n0.4166667\n0.2\n20.00000\n19.93092\n    40\n20\n0\n0.4166667\n0.3\n20.00023\n19.55791\n    40\n20\n0\n0.5000000\n0.1\n20.00000\n19.88493\n  \n  \n  \n\n\n\n\nInmediatamente procedemos a establecer el conjunto de datos de entrenamiento y testeo. Asi como establecemos la metodologia de V-fold validacion cruzada sobre el conjunto de datos de entrenamiento\n\n# 40 -60% \nsplit         &lt;- initial_split(option_prices, prop = 0.40)\noption_train  &lt;- training(split)\noption_test   &lt;- testing(split)\n\n\n\n\nset.seed(123)\noption_folds &lt;- vfold_cv(option_train, v = 20)\n\n\n\n\nLuego procedemos a plantear los potenciales predictores del precio de la opcion; asi como el procesamiento basico de los mismo o Feature engineering .Para ello empleamos una ‘recipe’\n\nrec.option &lt;- recipe(observed_price ~ .,\n              data = option_prices\n              ) |&gt;\n  step_rm(black_scholes) |&gt;\n  step_normalize(all_predictors())\n\n\n\n\nDefinimos el set de modelos que emplearemos y evaluaremos su performance\n\nmars_model &lt;- mars(  num_terms = tune(),\n                       prod_degree = tune(),\n                       prune_method = tune())  |&gt;\n                set_engine(\"earth\")            |&gt;\n                set_mode(\"regression\")\n\n\nlibrary(bonsai)\nrf_model &lt;-   rand_forest(mtry  = tune(), \n                           min_n  = tune(), trees = 100)            |&gt;\n              set_engine(\"ranger\")    |&gt;\n              set_mode(\"regression\")\n\n\n\n\nlgbm_model &lt;-   boost_tree(learn_rate = tune(), stop_iter = tune(),\n                            trees = 100) %&gt;%\n                set_engine(\"lightgbm\", num_leaves = tune()) %&gt;%\n                set_mode(\"regression\")\n\n\n\n\n\nmars_wflow   &lt;- workflow(rec.option, mars_model)\nrf_wflow     &lt;- workflow(rec.option, rf_model)\nlgbm_wflow   &lt;- workflow(rec.option, lgbm_model)\n\nLuego procedemos a ajustar el modelo planteado sobre los ‘folds’ creados, mediante la metodologia de validacion cruzada de manera que se busca optimizar los hiperparametros de los modelos planteados\n\nset.seed(123)\nmars_time_grid &lt;- system.time(\n                  mars_res_grid &lt;- tune_grid(mars_wflow, option_folds, grid = 5)\n)\n\n\nset.seed(123)\nrf_time_grid&lt;- system.time(\n                  rf_res_grid &lt;- tune_grid(rf_wflow, option_folds, grid = 5)\n)\n\n\nset.seed(123)\nlgbm_time_grid &lt;- system.time(\n                  lgbm_res_grid &lt;- tune_grid(lgbm_wflow, option_folds, grid = 5)\n)\n\n\n\n\nProcedemos a seleccionar el mejor modelo empleando para ello la metrica de error de la raiz cuadrada media estandarizada o rsme( por sus siglas en ingles )\n\nrf_rmse       &lt;- rf_res_grid %&gt;%\n                 select_best(\"rmse\", maximize = FALSE)\n\nlgbm_rmse     &lt;- lgbm_res_grid %&gt;%\n                 select_best(\"rmse\", maximize = FALSE)\n\nmars_rmse     &lt;- mars_res_grid %&gt;%\n                 select_best(\"rmse\", maximize = FALSE)\n\n\n\n\nfinal_mars &lt;- finalize_workflow(\n              mars_wflow,\n              mars_rmse\n)\n\nfinal_rf &lt;- finalize_workflow(\n              rf_wflow,\n              rf_rmse\n)\n\nfinal_lgbm &lt;- finalize_workflow(\n              lgbm_wflow,\n              lgbm_rmse\n)\n\n\n\nFinalmente reorganizamos los resultados de las metricas de error para facilitar la seleccion del mejor modelo.\n\nm_mars_db  &lt;- last_fit(\n              final_mars,\n              split\n              ) %&gt;%\n              collect_metrics() %&gt;% \n              mutate(.model =\"mars\")\n\n\nm_rf_db  &lt;- last_fit(\n            final_rf,\n            split\n            ) %&gt;%\n            collect_metrics() %&gt;% \n              mutate(.model =\"Random.Forest\")\n\nm_lgbm_db  &lt;- last_fit(\n              final_lgbm,\n              split\n              ) %&gt;%\n              collect_metrics()%&gt;% \n              mutate(.model =\"Light.gbm\")\n\n\nmtrcs_db   &lt;-rbind(m_mars_db,\n                   m_rf_db,\n                   m_lgbm_db)\n\n\nmtrcs_db                                                    |&gt; \nselect(.metric,.estimate,.model)                            |&gt; \npivot_wider(names_from = .metric,values_from =  .estimate)  |&gt; \ngt()                                                        |&gt;\ntab_header(\n    title = md(\"**Metricas de error**\"),\n    subtitle = \"Pricing opciones\"\n  )                                                         |&gt;\n  tab_options(\n    table.background.color = \"white\",\n    column_labels.background.color = \"white\",\n    table.font.size = px(15),\n    column_labels.font.size = px(17),\n    row.striping.background_color = \"gray\",\n    heading.align = \"center\",\n    heading.title.font.size = px(20)\n  ) %&gt;% \n  opt_row_striping() \n\n\n\n\n\n  \n    \n      Metricas de error\n    \n    \n      Pricing opciones\n    \n    \n      .model\n      rmse\n      rsq\n    \n  \n  \n    mars\n1.5005607\n0.9455582\n    Random.Forest\n0.2127804\n0.9989135\n    Light.gbm\n5.0715766\n0.9861009\n  \n  \n  \n\n\n\n\n\n\n\nPreprocesamiento del error de prediccion de cada modelo\n\nout_of_sample_data &lt;- testing(split)                  |&gt;\n                      slice_sample(n = 10000)\n\ndim(out_of_sample_data)\n\n[1] 10000     7\n\npredictive_performance &lt;- final_mars                  |&gt;         # best workflow\n                          fit(data = training(split)) |&gt;         # fitted with full training dataset\n                          predict(out_of_sample_data) |&gt;           # predict \n                          rename(\"mars\" = .pred)      |&gt; \n                          bind_cols(\n                              final_rf                      |&gt;         \n                              fit(data = training(split))   |&gt;         \n                              predict(out_of_sample_data)   |&gt;            \n                              rename(\"Random.Forest\" = .pred) \n                            )                               |&gt; \n                          bind_cols(\n                              final_rf                      |&gt;         \n                              fit(data = training(split))   |&gt;         \n                              predict(out_of_sample_data)   |&gt;            \n                              rename(\"LGBM\" = .pred) \n                            )                               |&gt;\n  bind_cols(out_of_sample_data)                             |&gt;\n  pivot_longer(\"mars\":\"LGBM\", names_to = \"Model\")           |&gt;\n    mutate(\n    moneyness = (S - K),\n    pricing_error = abs(value - black_scholes)\n  )\n\nVisualizacion final del error de prediccion de los modelos anteriormente evaluados\n\npredictive_performance |&gt;\n  ggplot(aes(\n    x = moneyness, \n    y = pricing_error, \n    color = Model,\n    linetype = Model\n    )) +\n  geom_jitter(alpha = 0.05) +\n  scale_color_viridis_d()+\n  geom_smooth(se = FALSE, method = \"gam\", formula = y ~ s(x, bs = \"cs\")) +\n  labs(\n    x = \"Moneyness (S - K)\", color = NULL,\n    y = \"Error de prediccion\",\n    title = \"Error de prediccion de modelos opciones de compra\",\n    linetype = NULL\n  )+theme_bw()+\n  theme(legend.position = \"bottom\",\n        plot.title = element_text(hjust = 0.5))"
  },
  {
    "objectID": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#carga-de-paquetes",
    "href": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#carga-de-paquetes",
    "title": "Aprendizaje automatico usando Tidymodels",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(gt)"
  },
  {
    "objectID": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#modelo-black--sholes",
    "href": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#modelo-black--sholes",
    "title": "Aprendizaje automatico usando Tidymodels",
    "section": "",
    "text": "Uno de los modelos clasicos para llevar acabo el pricing de opciones es el modelo black-sholes el cual tipicamente plantea la siguiente entidad :\n\\[ C(S_0,t) = S_0 N(d_1) - Ke^{-r(T-t)}N(d_2)\\]\n\\[ d_1 =     \\frac{(ln\\frac{S_{0}}{K} + (r +\\frac{σ^2}{2} )(T-t))}{σ    \\sqrt{T-t}} \\]\n\\[d_2 = d_1 - σ \\sqrt{T-t}\\]\nDonde:\n\n\\(S_0\\): Precio del subyacente (Stock Price)\n\\(C(S0,t)\\): Price of the Call Option\n\\(K\\): Exercise Price\n\\((T−t)\\): Tiempo de maduracion, donde T es la fecha de ejercicio(Time to Maturity, where T is Exercise Date)\n\\(σ\\): Volatilidad subyacente (Underlying Volatility (a standard deviation of log returns))\n\\(r\\): Tasa de interes libre de riesgo (Risk-free Interest Rate (i.e., T-bill Rate))\n\nEsta ecuacion puede ser formulada en R del siguiente modo :\n\nblack_scholes_price &lt;- function(S, K = 70, r = 0, T = 1, sigma = 0.2) {\n  \n  d1     &lt;- (log(S / K) + (r + sigma^2 / 2) * T) / (sigma * sqrt(T))\n  d2     &lt;-  d1 - sigma * sqrt(T)\n  price  &lt;-  S * pnorm(d1) - K * exp(-r * T) * pnorm(d2)\n  \n  return(price)\n}\n\nEmpleando la anterior expresion, podemos simular precios de opciones;asi como una base de datos de sus determinantes. Dado lo anterior, tendremos un escenario perfecto para llevar acabo metodologias asociadas al aprendizaje automatico o ML ya que podriamos calcular el predecir el precio de una opcion financiera . Asi partimos simulando la base de datos necesaria para el problema de regresion que se nos presenta\n\nset.seed(420)\n\noption_prices &lt;- expand_grid(\n  S = 40:50,\n  K = 20:40,\n  r = seq(from = 0, to = 0.03, by = 0.01),\n  T = seq(from = 3 / 12, to = 1.5, by = 1 / 12),\n  sigma = seq(from = 0.1, to = 0.3, by = 0.1)\n) |&gt;\n  mutate(\n    black_scholes = black_scholes_price(S, K, r, T, sigma),\n    observed_price = map_dbl(\n      black_scholes,\n      function(x) x + rnorm(1, sd = 0.15)\n    )\n  )\n\n  option_prices |&gt; \n  head(10)  |&gt;\n  gt() \n\n\n\n\n\n  \n    \n    \n      S\n      K\n      r\n      T\n      sigma\n      black_scholes\n      observed_price\n    \n  \n  \n    40\n20\n0\n0.2500000\n0.1\n20.00000\n20.04016\n    40\n20\n0\n0.2500000\n0.2\n20.00000\n19.85949\n    40\n20\n0\n0.2500000\n0.3\n20.00000\n20.08943\n    40\n20\n0\n0.3333333\n0.1\n20.00000\n19.95319\n    40\n20\n0\n0.3333333\n0.2\n20.00000\n20.05970\n    40\n20\n0\n0.3333333\n0.3\n20.00003\n19.92282\n    40\n20\n0\n0.4166667\n0.1\n20.00000\n19.91722\n    40\n20\n0\n0.4166667\n0.2\n20.00000\n19.93092\n    40\n20\n0\n0.4166667\n0.3\n20.00023\n19.55791\n    40\n20\n0\n0.5000000\n0.1\n20.00000\n19.88493\n  \n  \n  \n\n\n\n\nInmediatamente procedemos a establecer el conjunto de datos de entrenamiento y testeo. Asi como establecemos la metodologia de V-fold validacion cruzada sobre el conjunto de datos de entrenamiento\n\n# 40 -60% \nsplit         &lt;- initial_split(option_prices, prop = 0.40)\noption_train  &lt;- training(split)\noption_test   &lt;- testing(split)\n\n\n\n\nset.seed(123)\noption_folds &lt;- vfold_cv(option_train, v = 20)"
  },
  {
    "objectID": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#definicion-del-predictor",
    "href": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#definicion-del-predictor",
    "title": "Aprendizaje automatico usando Tidymodels",
    "section": "",
    "text": "Luego procedemos a plantear los potenciales predictores del precio de la opcion; asi como el procesamiento basico de los mismo o Feature engineering .Para ello empleamos una ‘recipe’\n\nrec.option &lt;- recipe(observed_price ~ .,\n              data = option_prices\n              ) |&gt;\n  step_rm(black_scholes) |&gt;\n  step_normalize(all_predictors())"
  },
  {
    "objectID": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#definicion-del-modelo-o-engine",
    "href": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#definicion-del-modelo-o-engine",
    "title": "Aprendizaje automatico usando Tidymodels",
    "section": "",
    "text": "Definimos el set de modelos que emplearemos y evaluaremos su performance\n\nmars_model &lt;- mars(  num_terms = tune(),\n                       prod_degree = tune(),\n                       prune_method = tune())  |&gt;\n                set_engine(\"earth\")            |&gt;\n                set_mode(\"regression\")\n\n\nlibrary(bonsai)\nrf_model &lt;-   rand_forest(mtry  = tune(), \n                           min_n  = tune(), trees = 100)            |&gt;\n              set_engine(\"ranger\")    |&gt;\n              set_mode(\"regression\")\n\n\n\n\nlgbm_model &lt;-   boost_tree(learn_rate = tune(), stop_iter = tune(),\n                            trees = 100) %&gt;%\n                set_engine(\"lightgbm\", num_leaves = tune()) %&gt;%\n                set_mode(\"regression\")"
  },
  {
    "objectID": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#definicion-de-los-flujos-de-trabajo",
    "href": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#definicion-de-los-flujos-de-trabajo",
    "title": "Aprendizaje automatico usando Tidymodels",
    "section": "",
    "text": "mars_wflow   &lt;- workflow(rec.option, mars_model)\nrf_wflow     &lt;- workflow(rec.option, rf_model)\nlgbm_wflow   &lt;- workflow(rec.option, lgbm_model)\n\nLuego procedemos a ajustar el modelo planteado sobre los ‘folds’ creados, mediante la metodologia de validacion cruzada de manera que se busca optimizar los hiperparametros de los modelos planteados\n\nset.seed(123)\nmars_time_grid &lt;- system.time(\n                  mars_res_grid &lt;- tune_grid(mars_wflow, option_folds, grid = 5)\n)\n\n\nset.seed(123)\nrf_time_grid&lt;- system.time(\n                  rf_res_grid &lt;- tune_grid(rf_wflow, option_folds, grid = 5)\n)\n\n\nset.seed(123)\nlgbm_time_grid &lt;- system.time(\n                  lgbm_res_grid &lt;- tune_grid(lgbm_wflow, option_folds, grid = 5)\n)"
  },
  {
    "objectID": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#evaluacion-del-mejor-modelo",
    "href": "Articles/2021-06-10_ML_Options/2021-06-10_ML_Options.html#evaluacion-del-mejor-modelo",
    "title": "Aprendizaje automatico usando Tidymodels",
    "section": "",
    "text": "Procedemos a seleccionar el mejor modelo empleando para ello la metrica de error de la raiz cuadrada media estandarizada o rsme( por sus siglas en ingles )\n\nrf_rmse       &lt;- rf_res_grid %&gt;%\n                 select_best(\"rmse\", maximize = FALSE)\n\nlgbm_rmse     &lt;- lgbm_res_grid %&gt;%\n                 select_best(\"rmse\", maximize = FALSE)\n\nmars_rmse     &lt;- mars_res_grid %&gt;%\n                 select_best(\"rmse\", maximize = FALSE)\n\n\n\n\nfinal_mars &lt;- finalize_workflow(\n              mars_wflow,\n              mars_rmse\n)\n\nfinal_rf &lt;- finalize_workflow(\n              rf_wflow,\n              rf_rmse\n)\n\nfinal_lgbm &lt;- finalize_workflow(\n              lgbm_wflow,\n              lgbm_rmse\n)\n\n\n\nFinalmente reorganizamos los resultados de las metricas de error para facilitar la seleccion del mejor modelo.\n\nm_mars_db  &lt;- last_fit(\n              final_mars,\n              split\n              ) %&gt;%\n              collect_metrics() %&gt;% \n              mutate(.model =\"mars\")\n\n\nm_rf_db  &lt;- last_fit(\n            final_rf,\n            split\n            ) %&gt;%\n            collect_metrics() %&gt;% \n              mutate(.model =\"Random.Forest\")\n\nm_lgbm_db  &lt;- last_fit(\n              final_lgbm,\n              split\n              ) %&gt;%\n              collect_metrics()%&gt;% \n              mutate(.model =\"Light.gbm\")\n\n\nmtrcs_db   &lt;-rbind(m_mars_db,\n                   m_rf_db,\n                   m_lgbm_db)\n\n\nmtrcs_db                                                    |&gt; \nselect(.metric,.estimate,.model)                            |&gt; \npivot_wider(names_from = .metric,values_from =  .estimate)  |&gt; \ngt()                                                        |&gt;\ntab_header(\n    title = md(\"**Metricas de error**\"),\n    subtitle = \"Pricing opciones\"\n  )                                                         |&gt;\n  tab_options(\n    table.background.color = \"white\",\n    column_labels.background.color = \"white\",\n    table.font.size = px(15),\n    column_labels.font.size = px(17),\n    row.striping.background_color = \"gray\",\n    heading.align = \"center\",\n    heading.title.font.size = px(20)\n  ) %&gt;% \n  opt_row_striping() \n\n\n\n\n\n  \n    \n      Metricas de error\n    \n    \n      Pricing opciones\n    \n    \n      .model\n      rmse\n      rsq\n    \n  \n  \n    mars\n1.5005607\n0.9455582\n    Random.Forest\n0.2127804\n0.9989135\n    Light.gbm\n5.0715766\n0.9861009\n  \n  \n  \n\n\n\n\n\n\n\nPreprocesamiento del error de prediccion de cada modelo\n\nout_of_sample_data &lt;- testing(split)                  |&gt;\n                      slice_sample(n = 10000)\n\ndim(out_of_sample_data)\n\n[1] 10000     7\n\npredictive_performance &lt;- final_mars                  |&gt;         # best workflow\n                          fit(data = training(split)) |&gt;         # fitted with full training dataset\n                          predict(out_of_sample_data) |&gt;           # predict \n                          rename(\"mars\" = .pred)      |&gt; \n                          bind_cols(\n                              final_rf                      |&gt;         \n                              fit(data = training(split))   |&gt;         \n                              predict(out_of_sample_data)   |&gt;            \n                              rename(\"Random.Forest\" = .pred) \n                            )                               |&gt; \n                          bind_cols(\n                              final_rf                      |&gt;         \n                              fit(data = training(split))   |&gt;         \n                              predict(out_of_sample_data)   |&gt;            \n                              rename(\"LGBM\" = .pred) \n                            )                               |&gt;\n  bind_cols(out_of_sample_data)                             |&gt;\n  pivot_longer(\"mars\":\"LGBM\", names_to = \"Model\")           |&gt;\n    mutate(\n    moneyness = (S - K),\n    pricing_error = abs(value - black_scholes)\n  )\n\nVisualizacion final del error de prediccion de los modelos anteriormente evaluados\n\npredictive_performance |&gt;\n  ggplot(aes(\n    x = moneyness, \n    y = pricing_error, \n    color = Model,\n    linetype = Model\n    )) +\n  geom_jitter(alpha = 0.05) +\n  scale_color_viridis_d()+\n  geom_smooth(se = FALSE, method = \"gam\", formula = y ~ s(x, bs = \"cs\")) +\n  labs(\n    x = \"Moneyness (S - K)\", color = NULL,\n    y = \"Error de prediccion\",\n    title = \"Error de prediccion de modelos opciones de compra\",\n    linetype = NULL\n  )+theme_bw()+\n  theme(legend.position = \"bottom\",\n        plot.title = element_text(hjust = 0.5))"
  },
  {
    "objectID": "Articles/2022-01-10_Time_series/Time_series.html",
    "href": "Articles/2022-01-10_Time_series/Time_series.html",
    "title": "Plotting with Ggplot2:Time series",
    "section": "",
    "text": "Plotting Time series with ggplot\n#Packages\n\nLoad Data\n\n\nplot"
  },
  {
    "objectID": "Articles/2024-03-16_Bayesian_Variable_Selection/Bayesian Variable Selection.html",
    "href": "Articles/2024-03-16_Bayesian_Variable_Selection/Bayesian Variable Selection.html",
    "title": "Bayesian variable selection",
    "section": "",
    "text": "Load required packages\n\nlibrary(wooldridge)\nlibrary(spikeslab)\nlibrary(dplyr)\n\n\n\nPreprocess our data\n\ndata(wage1, package = \"wooldridge\")\n\nwage1 &lt;- wage1 |&gt;\n         select(-wage)\n\n\n\nFit spikeslab regression\nThe spcikeslab regression model enable us to rank the determinants of wage accord to bma and gnet. The calculus of these concepts permits us to select the best drivers in the next steps.\n\nset.seed(123)\nobj &lt;- spikeslab(lwage ~ . , wage1)\nprint(obj)\n\n------------------------------------------------------------------- \nVariable selection method     : AIC \nBig p small n                 : FALSE \nScreen variables              : FALSE \nFast processing               : TRUE \nSample size                   : 526 \nNo. predictors                : 22 \nNo. burn-in values            : 500 \nNo. sampled values            : 500 \nEstimated mse                 : 0.1323 \nModel size                    : 22 \n\n\n---&gt; Top variables:\n            bma   gnet bma.scale gnet.scale\neduc      0.131  0.131     0.047      0.047\nfemale   -0.126 -0.132    -0.252     -0.265\ntenure    0.118  0.154     0.016      0.021\nexper     0.117  0.268     0.009      0.020\ntrade    -0.107 -0.130    -0.237     -0.287\nexpersq  -0.100 -0.250     0.000      0.000\nprofocc   0.100  0.106     0.207      0.219\nservices -0.070 -0.087    -0.231     -0.289\nsmsa      0.062  0.063     0.137      0.140\nservocc  -0.048 -0.038    -0.137     -0.110\nmarried   0.044  0.033     0.091      0.068\ntenursq  -0.025 -0.066     0.000      0.000\nwest      0.024  0.022     0.064      0.060\nnorthcen -0.016 -0.023    -0.038     -0.053\nndurman  -0.013 -0.029    -0.039     -0.090\nsouth    -0.010 -0.017    -0.021     -0.037\nprofserv -0.008 -0.032    -0.017     -0.073\nnumdep   -0.005 -0.019    -0.004     -0.015\ntrcommpu -0.005 -0.016    -0.024     -0.077\nclerocc   0.001  0.010     0.003      0.027\nconstruc -0.001 -0.008    -0.004     -0.039\nnonwhite  0.000 -0.001    -0.001     -0.004\n------------------------------------------------------------------- \n\n\nWe can improve this approach using CV-validation .\n\ncv.obj &lt;- cv.spikeslab(x = wage1 %&gt;% select(-lwage), y = wage1$lwage, K = 10)\n\n     K-fold: 1 \n     K-fold: 2 \n     K-fold: 3 \n     K-fold: 4 \n     K-fold: 5 \n     K-fold: 6 \n     K-fold: 7 \n     K-fold: 8 \n     K-fold: 9 \n     K-fold: 10 \n     final analysis (full-data)\n\n\n\n\n\n------------------------------------------------------------------- \nVariable selection method     : cross-validation \nBig p small n                 : FALSE \nScreen variables              : FALSE \nFast processing               : TRUE \nSample size                   : 526 \nNo. predictors                : 22 \nNo. burn-in values            : 500 \nNo. sampled values            : 500 \nK-fold                        : 10 \nCV mean-squared error         : 0.139 +/- 0.009 \nModel size                    : [12,23] \n\n\nTop variables in terms of stability:\n            bma bma.cv   gnet gnet.cv stability\neduc      0.131  0.131  0.131   0.131       100\nfemale   -0.126 -0.125 -0.133  -0.129       100\ntenure    0.113  0.118  0.151   0.138       100\ntrade    -0.106 -0.108 -0.129  -0.119       100\nprofocc   0.099  0.099  0.106   0.103       100\nservices -0.069 -0.070 -0.087  -0.078       100\nsmsa      0.062  0.061  0.063   0.060       100\nservocc  -0.049 -0.049 -0.038  -0.042       100\nmarried   0.044  0.044  0.033   0.036       100\nwest      0.024  0.024  0.023   0.023       100\nexper     0.122  0.112  0.272   0.206        90\nexpersq  -0.105 -0.095 -0.254  -0.189        90\ntenursq  -0.021 -0.026 -0.062  -0.048        90\nnorthcen -0.015 -0.017 -0.023  -0.017        90\nndurman  -0.011 -0.013 -0.028  -0.022        90\nsouth    -0.009 -0.010 -0.017  -0.013        90\nprofserv -0.007 -0.009 -0.031  -0.023        90\nnumdep   -0.006 -0.006 -0.020  -0.013        90\ntrcommpu -0.005 -0.006 -0.016  -0.012        80\nconstruc -0.001 -0.002 -0.008  -0.006        80\nclerocc   0.001  0.001  0.010   0.005        70\nnonwhite  0.000  0.000 -0.001  -0.001        70\n------------------------------------------------------------------- \n\n\nonce the model is fitted, we observed that we need only 10 drivers; on the other hand, the variables that contributes the most in terms of stability are the following :\n\ncv.stb    &lt;- as.data.frame(cv.obj$stability)\ngnet      &lt;- cv.stb$gnet\nstability &lt;- cv.stb$stability\n\nplot(gnet,stability,bty=\"n\")\ntext(gnet, stability, rownames(cv.obj$stability), pos=1,cex = 0.8\n     )"
  }
]