{
  "hash": "fa416aee98120ff6858f13c98ba61d40",
  "result": {
    "markdown": "---\ntitle: \"Bayesian variable selection\"\nsubtitle: \"Drivers of wage\"\ndescription: |\n  Description of how to apply the bayesian variable selection approach\ndate: \"2024-03-16\"\ncategories: [Plotting, data analysis,economics,bayesian Analysis]\nexecute:\n  message: false\n  warning: false\n---\n\n\n# Load required packages \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(wooldridge)\nlibrary(spikeslab)\nlibrary(dplyr)\n```\n:::\n\n\n# Preprocess our data\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(wage1, package = \"wooldridge\")\n\nwage1 <- wage1 |>\n         select(-wage)\n```\n:::\n\n\n\n# Fit spikeslab regression\n\nThe spcikeslab regression model enable us to rank the determinants of wage accord to bma and gnet. The calculus of these concepts permits us to select the best drivers in the next steps.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nobj <- spikeslab(lwage ~ . , wage1)\nprint(obj)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n------------------------------------------------------------------- \nVariable selection method     : AIC \nBig p small n                 : FALSE \nScreen variables              : FALSE \nFast processing               : TRUE \nSample size                   : 526 \nNo. predictors                : 22 \nNo. burn-in values            : 500 \nNo. sampled values            : 500 \nEstimated mse                 : 0.1323 \nModel size                    : 22 \n\n\n---> Top variables:\n            bma   gnet bma.scale gnet.scale\neduc      0.131  0.131     0.047      0.047\nfemale   -0.126 -0.132    -0.252     -0.265\ntenure    0.118  0.154     0.016      0.021\nexper     0.117  0.268     0.009      0.020\ntrade    -0.107 -0.130    -0.237     -0.287\nexpersq  -0.100 -0.250     0.000      0.000\nprofocc   0.100  0.106     0.207      0.219\nservices -0.070 -0.087    -0.231     -0.289\nsmsa      0.062  0.063     0.137      0.140\nservocc  -0.048 -0.038    -0.137     -0.110\nmarried   0.044  0.033     0.091      0.068\ntenursq  -0.025 -0.066     0.000      0.000\nwest      0.024  0.022     0.064      0.060\nnorthcen -0.016 -0.023    -0.038     -0.053\nndurman  -0.013 -0.029    -0.039     -0.090\nsouth    -0.010 -0.017    -0.021     -0.037\nprofserv -0.008 -0.032    -0.017     -0.073\nnumdep   -0.005 -0.019    -0.004     -0.015\ntrcommpu -0.005 -0.016    -0.024     -0.077\nclerocc   0.001  0.010     0.003      0.027\nconstruc -0.001 -0.008    -0.004     -0.039\nnonwhite  0.000 -0.001    -0.001     -0.004\n------------------------------------------------------------------- \n```\n:::\n:::\n\n\nWe can improve this approach using CV-validation .\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncv.obj <- cv.spikeslab(x = wage1 %>% select(-lwage), y = wage1$lwage, K = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\t K-fold: 1 \n\t K-fold: 2 \n\t K-fold: 3 \n\t K-fold: 4 \n\t K-fold: 5 \n\t K-fold: 6 \n\t K-fold: 7 \n\t K-fold: 8 \n\t K-fold: 9 \n\t K-fold: 10 \n\t final analysis (full-data)\n```\n:::\n\n::: {.cell-output-display}\n![](Bayesian-Variable-Selection_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n------------------------------------------------------------------- \nVariable selection method     : cross-validation \nBig p small n                 : FALSE \nScreen variables              : FALSE \nFast processing               : TRUE \nSample size                   : 526 \nNo. predictors                : 22 \nNo. burn-in values            : 500 \nNo. sampled values            : 500 \nK-fold                        : 10 \nCV mean-squared error         : 0.139 +/- 0.009 \nModel size                    : [12,23] \n\n\nTop variables in terms of stability:\n            bma bma.cv   gnet gnet.cv stability\neduc      0.131  0.131  0.131   0.131       100\nfemale   -0.126 -0.125 -0.133  -0.129       100\ntenure    0.113  0.118  0.151   0.138       100\ntrade    -0.106 -0.108 -0.129  -0.119       100\nprofocc   0.099  0.099  0.106   0.103       100\nservices -0.069 -0.070 -0.087  -0.078       100\nsmsa      0.062  0.061  0.063   0.060       100\nservocc  -0.049 -0.049 -0.038  -0.042       100\nmarried   0.044  0.044  0.033   0.036       100\nwest      0.024  0.024  0.023   0.023       100\nexper     0.122  0.112  0.272   0.206        90\nexpersq  -0.105 -0.095 -0.254  -0.189        90\ntenursq  -0.021 -0.026 -0.062  -0.048        90\nnorthcen -0.015 -0.017 -0.023  -0.017        90\nndurman  -0.011 -0.013 -0.028  -0.022        90\nsouth    -0.009 -0.010 -0.017  -0.013        90\nprofserv -0.007 -0.009 -0.031  -0.023        90\nnumdep   -0.006 -0.006 -0.020  -0.013        90\ntrcommpu -0.005 -0.006 -0.016  -0.012        80\nconstruc -0.001 -0.002 -0.008  -0.006        80\nclerocc   0.001  0.001  0.010   0.005        70\nnonwhite  0.000  0.000 -0.001  -0.001        70\n------------------------------------------------------------------- \n```\n:::\n:::\n\n\nonce the model is fitted, we observed that we need only 10 drivers; on the other hand, the variables that contributes the most in terms of stability are the following : \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncv.stb    <- as.data.frame(cv.obj$stability)\ngnet      <- cv.stb$gnet\nstability <- cv.stb$stability\n\nplot(gnet,stability,bty=\"n\")\ntext(gnet, stability, rownames(cv.obj$stability), pos=1,cex = 0.8\n     )\n```\n\n::: {.cell-output-display}\n![](Bayesian-Variable-Selection_files/figure-html/unnamed-chunk-5-1.png){width=1440}\n:::\n:::\n",
    "supporting": [
      "Bayesian-Variable-Selection_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}